{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eca7c7d",
   "metadata": {
    "editable": false,
    "id": "4c4eeb3b",
    "lang": "fr",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Devoir 4, Question 3 : Transfert de style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0347c",
   "metadata": {
    "editable": false,
    "id": "2fa8cc0c",
    "lang": "en",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Homework 4, Question 3: Style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bbbe6",
   "metadata": {
    "editable": false,
    "id": "7642ba71",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Code préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b320c",
   "metadata": {
    "editable": false,
    "id": "b9c84a75",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Preamble code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55398c74",
   "metadata": {
    "editable": false,
    "id": "b84b5c45",
    "tags": [
     "problem-context",
     "autoexec"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 0)\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Constantes / Constants\n",
    "IMG_SIZE = 256\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406] # Moyenne pour chaque canal de couleur\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]  # Std pour chaque canal de couleur\n",
    "STYLE_IMAGE = 'style_image'\n",
    "CONTENT_IMAGE = 'content_image'\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Variables\n",
    "results = {'Name':[],\n",
    "           'Shape':[],\n",
    "           'Mean':[],\n",
    "           'Std':[],\n",
    "          }\n",
    "\n",
    "def fetch_image(file_id):\n",
    "    \"\"\"\n",
    "    Cette fonction télécharge une image que vous partagez de votre Google Drive.\n",
    "    Elle retourne l'image dans un format PIL.\n",
    "    This function downloads an image you share from your Google Drive.\n",
    "    It returns the image in a PIL format.\n",
    "    \"\"\"\n",
    "    URL = \"https://drive.google.com/uc?\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    r = session.get(URL, params = { 'id' : file_id, 'alt' : 'media'}, stream = True)\n",
    "    error_msg = f'ERROR: impossible to download the image (code={r.status_code})'\n",
    "    assert(r.status_code == 200), error_msg\n",
    "    \n",
    "    params = { 'id' : file_id, 'confirm' : 'download_warning' }\n",
    "    r = session.get(URL, params = params, stream = True)\n",
    "    stream = BytesIO(r.content)\n",
    "    image = Image.open(stream)\n",
    "    return image\n",
    "    \n",
    "# Gram matrix\n",
    "def gram_matrix(tensor):\n",
    "    \"\"\"\n",
    "    Calcul de la matrice de Gram pour un tenseur donné\n",
    "    Calculation of the Gram matrix for a given tensor \n",
    "    Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the (B, C, H, W) of the Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    # Reshape tensor to multiply the features for each channel\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    # Calculate the Gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram\n",
    "\n",
    "def extract_features(image, model_features, layers=None):\n",
    "    \"\"\"\n",
    "    Infère l'image dans le modèle et extrait les features pour\n",
    "    les couches désirées. Les couches par défaut concordent\n",
    "    avec celles du réseau VGG19 de Gatys et al. (2016).\n",
    "    Infers the image into the model and extracts the features for\n",
    "    the desired layers. The default layers are consistent with\n",
    "    those of the VGG19 network of Gatys et al. (2016).\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '2': 'conv1_2',\n",
    "                  '5': 'conv2_1',\n",
    "                  '7': 'conv2_2',\n",
    "                  '10': 'conv3_1',\n",
    "                  '12': 'conv3_2',\n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',\n",
    "                  '28': 'conv5_1',\n",
    "                  '30': 'conv5_2'}\n",
    "                \n",
    "    features = {}\n",
    "    x = image\n",
    "    for layer_idx, layer in enumerate(model_features):\n",
    "        x = layer(x)\n",
    "        if str(layer_idx) in layers:\n",
    "            features[layers[str(layer_idx)]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245fdd3",
   "metadata": {
    "editable": false,
    "id": "f1e54722",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "De manière générale, l'entraînement des réseaux de neurones en classement implique un entraînement où on observe la performance selon sa fonction de perte que l'on souhaite minimiser en validation, afin d'obtenir un modèle qui généralise bien. Le classement n'est pas le seul contexte d'apprentissage pour les réseaux de neurones, il existe plusieurs autres utilisations de réseaux de neurones, notamment pour permettre la génération d'images. Le contexte de génération d'images offre ainsi un retour visuel qui permet de donner une appréciation qualitative du fonctionnement du système. Si les images générées semblent réelles, on peut présumer que le modèle fonctionne bien! L'exercice suivant a alors été conçu pour vous permettre de mieux visualiser les performances, avec un retour visuel qui devrait être évocateur. \n",
    "\n",
    "À l'aide de PyTorch, vous allez mettre en application ce qu'on appelle le *transfert de style*, qui est un problème qui fait appel à la notion de transfert de représentation. À l'aide d'un réseau VGG19, on vous demande de suivre l'implémentation d'un article de recherche [Gatys et coll., 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) afin de générer une image artistique. En utilisant un modèle fourni préentraîné sur ImageNet, nous voulons extraire le *style* d'une première image (à gauche) avec le réseau et l'appliquer sur le *contenu* d'une seconde image (à droite):\n",
    "\n",
    "![Contenu et style, Mona-Lisa](https://pax.ulaval.ca/static/GIF-4101-7005/images/content_style_mona.png)\n",
    "\n",
    "L'objectif est de créer une image hybride qui contient à la fois le style ainsi que le contenu de deux images. Il s'agit d'un exercice plus visuel qui vous permettra d'améliorer votre intuition sur le fonctionnement d'un réseau de neurones à convolution en vous basant sur l'information d'un article scientifique. Pour vous donner une idée du résultat, voici l'image hybride générée à partir des images présentées plus haut:\n",
    "\n",
    "![Transfert Mona-Lisa](https://pax.ulaval.ca/static/GIF-4101-7005/images/style_transfer_mona.gif)\n",
    "\n",
    "Le réseau de neurones à convolution VGG19 est composé de 2 groupes: les *features* et les couches de classification. Le style correspond au résultat du passage des filtres de la couche de *features* sur l'image d'entrée, alors que le contenu correspond aux valeurs des pixels de la deuxième image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d181dffe",
   "metadata": {
    "editable": false,
    "id": "dd763e44",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "In general, training neural networks for classification involves training them to observe performance according to its loss function, which we wish to minimize in validation, in order to obtain a model that generalizes well. Classification is not the only training context for neural networks. They have several other uses, notably image generation. The context of image generation thus provides visual feedback that gives a qualitative assessment of the system's operation. If the generated images look real, we can assume that the model works well! The following exercise is designed to allow you to better visualize the performance, with visual feedback that should be evocative. \n",
    "\n",
    "Using PyTorch, you will implement what is called *style transfer*, which is a problem that involves the notion of representation transfer. Using a VGG19 network, you are asked to follow the implementation of a research paper [Gatys et al., 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) to generate an artistic image. Using a provided template pre-trained on ImageNet, we want to extract the *style* of a first image (left) with the network and apply it to the *content* of a second image (right):\n",
    "\n",
    "![Content and style, Mona-Lisa](https://pax.ulaval.ca/static/GIF-4101-7005/images/content_style_mona.png)\n",
    "\n",
    "The objective is to create a hybrid image that contains both the style and the content of two images. This is a more visual exercise that will allow you to improve your intuition on how a convolution neural network works based on information from a scientific article. To give you an idea of the result, here is the hybrid image generated from the images presented above:\n",
    "\n",
    "![Mona-Lisa transfer](https://pax.ulaval.ca/static/GIF-4101-7005/images/style_transfer_mona.gif)\n",
    "\n",
    "The VGG19 convolution neural network is composed of 2 groups: the *features* and the classification layers. The style corresponds to the result of passing the filters of the *features* layers on the input image, while the content corresponds to the pixel values of the second image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb47bb",
   "metadata": {
    "editable": false,
    "id": "e8c62ac2",
    "lang": "fr"
   },
   "source": [
    "## Q3A\n",
    "Le transfert de style ne demande que deux images comme jeu de données pour son application:\n",
    "- Une image qui contient un style que vous souhaitez extraire (`style_image`);\n",
    "- Une image de contenu sur laquelle vous souhaitez appliquer le style (`content_image`).\n",
    "\n",
    "L'objectif du réseau est d'optimiser les pixels de l'image hybride en pondérant le style et le contenu des images sources. Il est à noter que le réseau utilisé, VGG19, doit préalablement être entraîné sur un très grand nombre de données. Toutefois, puisqu'on utilise un réseau préentraîné, vous n'avez pas besoin de toutes ces images pour son entraînement.\n",
    "\n",
    "La première étape du problème est de télécharger vos images de *style* et de *contenu*. Avec l'aide de la fonction `fetch_image`, vous pouvez télécharger vos propres images à partir de votre Google Drive. Pour se faire, téléversez une image de style ainsi qu'une image de contenu sur votre Google Drive et partagez-les publiquement en créant un lien URL de partage. Le lien aura la forme suivante:\n",
    "\n",
    "`https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing`\n",
    "\n",
    "Copiez-collez le `<FILE_ID>` et passez-le comme chaîne de caractères en entrée de la fonction `fetch_image('<FILE_ID>')` pour télécharger votre image dans le notebook.\n",
    "\n",
    "Quelques exemples d'images de contenu:\n",
    "- [Great Sea Turtle](https://drive.google.com/file/d/11c650QrD0vP7le1EHiZ5nkjRuoUYmF6H/view?usp=sharing) (`<FILE_ID> : 11c650QrD0vP7le1EHiZ5nkjRuoUYmF6H`)\n",
    "- [Tuebingen](https://drive.google.com/file/d/11ec7XKIPQXVq6jq0Swq96abJ3t4r6JQV/view?usp=sharing) (`<FILE_ID> : 11ec7XKIPQXVq6jq0Swq96abJ3t4r6JQV`)\n",
    "- [Grace Hopper](https://drive.google.com/file/d/11hj6wRTK3LvfNH1H2eGZCRAFA_h-f3Ag/view?usp=sharing) (`<FILE_ID> : 11hj6wRTK3LvfNH1H2eGZCRAFA_h-f3Ag`)\n",
    "\n",
    "Quelques exemples d'images de style:\n",
    "- [The Great Wave off Kanagawa](https://drive.google.com/file/d/11lRkyOtVCSZFrYT5r44y1rYXlywOmdaU/view?usp=sharing) (`<FILE_ID> : 11lRkyOtVCSZFrYT5r44y1rYXlywOmdaU`)\n",
    "- [Kadinsky](https://drive.google.com/file/d/11utiecLh-3JQspwfOVHowkoWOHsD4Zx5/view?usp=sharing) (`<FILE_ID> : 11utiecLh-3JQspwfOVHowkoWOHsD4Zx5`)\n",
    "- [Van Gogh](https://drive.google.com/file/d/11vgRvxUxwh8Q5uwaD8O9aYKO7yucm57A/view?usp=sharing) (`<FILE_ID> : 11vgRvxUxwh8Q5uwaD8O9aYKO7yucm57A`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346b794",
   "metadata": {
    "editable": false,
    "id": "9396ffed",
    "lang": "en"
   },
   "source": [
    "## Q3A\n",
    "Style transfer requires only two images as dataset for its application:\n",
    "- An image that contains a style you want to extract (`style_image`);\n",
    "- A content image on which you want to apply the style (`content_image`).\n",
    "\n",
    "The objective of the network is to optimize the pixels of the hybrid image by weighting the style and content of the source images. It should be noted that the network used, VGG19, must first be trained on a very large amount of data. However, since a pre-trained network is used, you do not need all these images for its training.\n",
    "\n",
    "The first step in the problem is to download your *style* and *content* images. With the help of the `fetch_image` function, you can upload your own from your Google Drive. To do so, upload a style image and a content image to your Google Drive and share them publicly by creating a share URL link. The link will look like this:\n",
    "\n",
    "`https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing`\n",
    "\n",
    "Copy and paste the `<FILE_ID>` and pass it as a string as input to the `fetch_image('<FILE_ID>')` function to upload your image to the notebook.\n",
    "\n",
    "Some examples of content images:\n",
    "- [Great Sea Turtle](https://drive.google.com/file/d/11c650QrD0vP7le1EHiZ5nkjRuoUYmF6H/view?usp=sharing) (`<FILE_ID> : 11c650QrD0vP7le1EHiZ5nkjRuoUYmF6H`)\n",
    "- [Tuebingen](https://drive.google.com/file/d/11ec7XKIPQXVq6jq0Swq96abJ3t4r6JQV/view?usp=sharing) (`<FILE_ID> : 11ec7XKIPQXVq6jq0Swq96abJ3t4r6JQV`)\n",
    "- [Grace Hopper](https://drive.google.com/file/d/11hj6wRTK3LvfNH1H2eGZCRAFA_h-f3Ag/view?usp=sharing) (`<FILE_ID> : 11hj6wRTK3LvfNH1H2eGZCRAFA_h-f3Ag`)\n",
    "\n",
    "Some examples of style images:\n",
    "- [The Great Wave off Kanagawa](https://drive.google.com/file/d/11lRkyOtVCSZFrYT5r44y1rYXlywOmdaU/view?usp=sharing) (`<FILE_ID> : 11lRkyOtVCSZFrYT5r44y1rYXlywOmdaU`)\n",
    "- [Kadinsky](https://drive.google.com/file/d/11utiecLh-3JQspwfOVHowkoWOHsD4Zx5/view?usp=sharing) (`<FILE_ID> : 11utiecLh-3JQspwfOVHowkoWOHsD4Zx5`)\n",
    "- [Van Gogh](https://drive.google.com/file/d/11vgRvxUxwh8Q5uwaD8O9aYKO7yucm57A/view?usp=sharing) (`<FILE_ID> : 11vgRvxUxwh8Q5uwaD8O9aYKO7yucm57A`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251a2d1",
   "metadata": {
    "editable": false,
    "id": "70055a9c",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525bee8",
   "metadata": {
    "editable": false,
    "id": "30c27edc",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3A answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0f443",
   "metadata": {
    "editable": false,
    "id": "b0039404",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avec la fonction fetch_image, téléchargez vos propres images.\n",
    "# Pour se faire, ajoutez vos images sur votre Google Drive en les\n",
    "# téléversant et partagez-les avec un lien URL. Dans ce lien, \n",
    "# vous retrouverez le <FILE_ID> qu'il faut copier-coller dans la\n",
    "# fonction fetch_image.\n",
    "# Exemple: https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing\n",
    "# With the fetch_image function, upload your own images.\n",
    "# To do so, add your images to your Google Drive by\n",
    "# uploading them and share them with a URL link. In this link, \n",
    "# you will find the <FILE_ID> that you need to copy and paste into the\n",
    "# function fetch_image.\n",
    "# Example: https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing\n",
    "\n",
    "# *** TODO ***\n",
    "# Télécharger une image contenant le style à extraire\n",
    "# Download an image containing the style to extract\n",
    "style_image_file_id = \"\"\n",
    "# ******\n",
    "style_image = fetch_image(style_image_file_id)\n",
    "\n",
    "# *** TODO **\n",
    "# Télécharger une image sur laquelle appliquer le style\n",
    "# Download an image on which to apply the style\n",
    "content_image_file_id = \"\"\n",
    "# ******\n",
    "content_image = fetch_image(content_image_file_id)\n",
    "\n",
    "images = {STYLE_IMAGE:style_image,\n",
    "          CONTENT_IMAGE:content_image}\n",
    "\n",
    "# Afficher les 2 images côte-à-côte\n",
    "# Display the 2 images side by side\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# Affichage du style_image\n",
    "# Displaying the image_style\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[STYLE_IMAGE])\n",
    "\n",
    "# Affichage du content_image\n",
    "# Displaying the content_image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(images[CONTENT_IMAGE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040bbd7",
   "metadata": {
    "editable": false,
    "id": "594abc8e",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3A dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea241b73",
   "metadata": {
    "editable": false,
    "id": "2d37c06a",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3A in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9969f",
   "metadata": {
    "editable": false,
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "<div class=\"feedback-cell\" style=\"background: rgba(100 , 100 , 100 , 0.4)\">\n",
    "                <h3>Votre soumission a été enregistrée!</h3>\n",
    "                <ul>\n",
    "                    <li>notez qu'il n'y a <strong>pas</strong> de correction automatique pour cet exercice&puncsp;;</li>\n",
    "                    <li>par conséquent, votre note est <strong>actuellement</strong> zéro&puncsp;;</li>\n",
    "                    <li>elle sera cependant ajustée par le professeur dès que la correction manuelle sera complétée&puncsp;;</li>\n",
    "                    <li>vous pouvez soumettre autant de fois que nécessaire jusqu'à la date d'échéance&puncsp;;</li>\n",
    "                    <li>mais évitez de soumettre inutilement.</li>\n",
    "                </ul>\n",
    "            </div><p class=\"alert alert-warning\"><strong>ATTENTION</strong>: cette soumission a été effectuée <strong>après</strong> l'échéance, elle ne sera <strong>pas</strong> considérée.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06fb5",
   "metadata": {
    "deletable": false,
    "id": "7c3d044c",
    "tags": [
     "user-answer-D4Q3A",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Avec la fonction fetch_image, téléchargez vos propres images.\n",
    "# Pour se faire, ajoutez vos images sur votre Google Drive en les\n",
    "# téléversant et partagez-les avec un lien URL. Dans ce lien, \n",
    "# vous retrouverez le <FILE_ID> qu'il faut copier-coller dans la\n",
    "# fonction fetch_image.\n",
    "# Exemple: https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing\n",
    "# With the fetch_image function, upload your own images.\n",
    "# To do so, add your images to your Google Drive by\n",
    "# uploading them and share them with a URL link. In this link, \n",
    "# you will find the <FILE_ID> that you need to copy and paste into the\n",
    "# function fetch_image.\n",
    "# Example: https://drive.google.com/file/d/<FILE_ID>/view?usp=sharing\n",
    "\n",
    "# *** TODO ***\n",
    "# Télécharger une image contenant le style à extraire\n",
    "# Download an image containing the style to extract\n",
    "style_image_file_id = \"1hmNO6kDm1Bc3rgA2x1PGYi8BB_i4ABXp\"\n",
    "# ******\n",
    "style_image = fetch_image(style_image_file_id)\n",
    "\n",
    "# *** TODO **\n",
    "# Télécharger une image sur laquelle appliquer le style\n",
    "# Download an image on which to apply the style\n",
    "content_image_file_id = \"1GYQx4xI8pPjH9AUaveDFGNWkPyGC1IMH\"\n",
    "# ******\n",
    "content_image = fetch_image(content_image_file_id)\n",
    "\n",
    "images = {STYLE_IMAGE:style_image,\n",
    "          CONTENT_IMAGE:content_image}\n",
    "\n",
    "# Afficher les 2 images côte-à-côte\n",
    "# Display the 2 images side by side\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# Affichage du style_image\n",
    "# Displaying the image_style\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[STYLE_IMAGE])\n",
    "\n",
    "# Affichage du content_image\n",
    "# Displaying the content_image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(images[CONTENT_IMAGE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b2729",
   "metadata": {
    "editable": false,
    "id": "23f895e4",
    "lang": "fr"
   },
   "source": [
    "## Q3B\n",
    "#### Prétraitement\n",
    "Le prétraitement des images est nécessaire pour s'assurer que celles-ci aient les mêmes caractéristiques (taille, intensité moyenne, etc.) que celles des images utilisées pour l'entraînement. Il est également utilisé pour faire de l'augmentation de jeu de données, en ajoutant de la diversité dans le jeu d'entraînement pour augmenter le nombre d'images disponibles. Dans le cas du transfert de style, on l'utilise pour que les images respectent la même distribution que pour l'entraînement du réseau VGG19 utilisé.\n",
    "\n",
    "Puisqu'un réseau préentraîné est utilisé pour le transfert de style, il est important d'appliquer les mêmes paramètres de normalisation que ceux utilisés pour l'entraînement. Le CNN ayant été entraîné sur *ImageNet*, on applique les mêmes paramètres que dans la [documentation](https://pytorch.org/vision/stable/models.html). Il est à noter que ces paramètres représentent la moyenne et la déviation standard pour chaque canal de l'image. Une image standard de type RGB dispose de 3 canaux (Red, Green, Blue).\n",
    "* ImageNet_mean = [0.485, 0.456, 0.406]\n",
    "* ImageNet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#### Post-traitement\n",
    "Le post-traitement est nécessaire pour s'assurer que les images qui sont générées par le réseau de neurones respectent les propriétés naturelles d'une image réelle pour pouvoir être affichées avec *Matplotlib*. Une image standard de type RGB contient trois canaux de couleurs qui sont composés de pixels. L'intensité de la couleur de chacun des pixels se trouve dans une plage [0,1]. Toutefois, rien ne garantit que les pixels inférés par le réseau de neurones respecteront cette plage. En effet, comme le réseau VGG19 fait usage de la fonction d'activation sigmoïde, les pixels en sortie sont contenus entre [-1,1] et doivent donc être ramenés entre [0,1]. Il va de même pour la normalisation. Comme le réseau VGG19 a été préentraîné sur ImageNet avec des paramètres de normalisation spécifique à ce jeu de données, on normalise les images de style et de contenu de la même manière en prétraitement. Toutefois, afin d'afficher l'image hybride, il est important de renverser la normalisation pour obtenir un résultat visuellement intéressant.\n",
    "\n",
    "#### Objectif\n",
    "À partir du patron de prétraitement qui vous est donné et de la classe maison `AddDimension` qui vous permet d'ajouter une dimension, vous devez implémenter les transformations inverses (post-traitement) afin d'annuler les transformations qui ont été faites en amont de l'optimisation. Pour se faire, vous devez implémenter les modules de transformations suivants:\n",
    "1. `RemoveDimension`: Retirer la dimension *B* de la *batch_size* (taille de lot). On veut que le format passe de (B,C,H,W) -> (C,H,W). Utilisez la fonction *PyTorch* `squeeze`.\n",
    "2. `DeNormalize`: Retirer la normalisation en appliquant son inverse sur les valeurs des pixels de l'image. On veut annuler l'effet de *ImageNet_mean* et de *ImageNet_std* Vous devez faire les manipulations manuellement.\n",
    "3. `Clamp`: Fixer les valeurs des pixels de l'image dans les bornes [0,1]. Utilisez la fonction *PyTorch* `clamp`.\n",
    "4. `Permute`: Faire une permutation de l'ordre des dimensions pour permettre à la librairie Matplotlib de lire l'image. On veut que le format passe de (C,H,W) -> (H,W,C). Utilisez la fonction *PyTorch* `permute`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c80080",
   "metadata": {
    "editable": false,
    "id": "e3374fcf",
    "lang": "en"
   },
   "source": [
    "## Q3B\n",
    "#### Preprocessing\n",
    "Image preprocessing is necessary to ensure that the images have the same characteristics as the images used for training. With this preprocessing, we aim to bring the images to the same dimension, normalize the intensities, etc. It is also used to increase the data set, by adding diversity to the training set and thus multiply the number of images available. In the case of style transfer, it is used so that the images respect the same distribution as for the training of the VGG19 network you will use.\n",
    "\n",
    "Since a pre-trained network is used for the style transfer, it is important to apply the same normalization parameters as those used for training. Since the CNN was trained on *ImageNet*, we apply the same parameters as in the [documentation](https://pytorch.org/vision/stable/models.html). Note that these parameters represent the mean and standard deviation for each channel of the image. A standard RGB image has 3 channels (Red, Green, Blue).\n",
    "* ImageNet_mean = [0.485, 0.456, 0.406]\n",
    "* ImageNet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#### Postprocessing\n",
    "Postprocessing is necessary to ensure that the images that are generated by the neural network respect the natural properties of a real image to be displayed with *Matplotlib*. A standard RGB image contains three color channels that are composed of pixels. The color intensity of each pixel is in a range [0,1]. However, there is no guarantee that the pixels inferred by the neural network will respect this range. Indeed, since the VGG19 network makes use of the sigmoid activation function, the output pixels are in the range [-1,1] and need to be brought back to [0,1]. The same applies to the normalization. Since the VGG19 network has been pre-trained on ImageNet with normalization parameters specific to this dataset, we normalize the style and content images in the same way in preprocessing. However, in order to display the hybrid image, it is important to reverse the normalization if we want to have a visually interesting result.\n",
    "\n",
    "#### Objective\n",
    "From the given preprocessing pattern and the custom class `AddDimension` which allows you to add a dimension, you have to implement the reverse transformations (postprocessing) in order to undo the transformations that have been done before the optimization. To do this, you need to implement the following transformation modules:\n",
    "1. `RemoveDimension`: Remove the *B* dimension from the *batch_size*. We want the format to change from (B,C,H,W) -> (C,H,W). Use the *PyTorch* function `squeeze`.\n",
    "2. `DeNormalize`: Remove the normalization by applying its inverse to the pixel values of the image. We want to cancel the effect of *ImageNet_mean* and *ImageNet_std*.\n",
    "3. `Clamp`: Set the values of the pixels of the image in the bounds [0,1]. Use the *PyTorch* `clamp` function.\n",
    "4 `Permute`: Permute the order of the dimensions to allow the Matplotlib library to read the image. We want the format to change from (C,H,W) -> (H,W,C). Use the *PyTorch* function `permute`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbaadf",
   "metadata": {
    "editable": false,
    "id": "090f66d2",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6304e4",
   "metadata": {
    "editable": false,
    "id": "23d13379",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3B answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654eb06",
   "metadata": {
    "editable": false,
    "id": "6c5d18d7"
   },
   "outputs": [],
   "source": [
    "# Classe de transformation Custom pour ajouter un channel à la position \"dim\".\n",
    "# Cette classe vous est donnée comme exemple pour l'implémentation des autres transformations.\n",
    "# Custom transformation class to add a channel at the \"dim\" position.\n",
    "# This class is given as an example for the implementation of other transformations.\n",
    "class AddDimension(object):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C,H,W).\n",
    "\n",
    "        Returns:\n",
    "            tensor (Tensor): Tensor image with an added channel, now of size (1,C,H,W).\n",
    "        \"\"\"\n",
    "\n",
    "        new_x = x.unsqueeze(self.dim)\n",
    "        return new_x\n",
    "    \n",
    "# Étapes de prétraitement\n",
    "# 1. Redimensionner l'images à la taille désirée -> (3, 256, 256)\n",
    "# 2. Transformer l'image PIL en tenseur\n",
    "# 3. Appliquer la normalisation ImageNet\n",
    "# 4. Ajouter une dimension pour PyTorch (C,H,W) -> (B,C,H,W)\n",
    "#    où B est la taille de batch (lot).\n",
    "#\n",
    "# Preprocessing steps\n",
    "# Resize the image to the desired size -> (3, 256, 256)\n",
    "# 2. Transform the PIL image into a tensor\n",
    "# 3. Apply ImageNet normalization\n",
    "# 4. Add a dimension for PyTorch (C,H,W) -> (B,C,H,W)\n",
    "# where B is the batch size.\n",
    "preprocessing = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean=IMAGENET_MEAN,\n",
    "                                                         std=IMAGENET_STD),\n",
    "                                    AddDimension(0),\n",
    "                                   ])\n",
    "\n",
    "\n",
    "# Puisque PyTorch travaille sur des lots (batch)\n",
    "# d'images, une dimension supplémentaire (B) est\n",
    "# ajoutée pour son fonctionnement. L'image en entrée\n",
    "# passe donc de la taille (3, 256, 256) à la taille\n",
    "# (B, 3, 256, 256) où B, ici, est de taille 1, car il\n",
    "# n'y a qu'une seule image par lot.\n",
    "# \n",
    "# Since PyTorch works on batches of images,\n",
    "# an extra dimension (B) is added for its operation. The input\n",
    "# image goes from the size (3, 256, 256) to the size\n",
    "# (B, 3, 256, 256) where B, here, is of size 1, because\n",
    "# there is only one image per batch.\n",
    "#\n",
    "# Toutefois, afin d'afficher l'image hybride, il est important\n",
    "# de retirer cette dimension supplémentaire, car les outils\n",
    "# d'affichage s'attendent à afficher une image unique\n",
    "# Cette prochaine classe doit donc vous permettre de\n",
    "# retirer cette dimension supplémentaire. \n",
    "#\n",
    "# However, in order to display the hybrid image, it is important\n",
    "# to remove this extra dimension, because the display tools\n",
    "# expect to display a single image. \n",
    "# This next class should allow you to \n",
    "# remove this extra dimension. \n",
    "class RemoveDimension(object):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor image of size (1,C,H,W).\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor image with the removed channel, now of size (C,H,W).\n",
    "        \"\"\"\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom\n",
    "        # pour retirer un channel à la position \"dim\"\n",
    "        # Utilisez la fonction Pytorch Squeeze()\n",
    "        # Implementation of a Custom transformation\n",
    "        # to remove a channel at the \"dim\" position\n",
    "        # Use the Pytorch Squeeze() function\n",
    "        return new_x  # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "# Comme le réseau VGG19 a été pré-entraîné sur ImageNet\n",
    "# avec des paramètres de normalisation spécifique à ce\n",
    "# jeu de données, on assume qu'il sera plus performant sur\n",
    "# une nouvelle distribution d'images si celle-ci partage\n",
    "# également cette normalisation. Ainsi, pour l'extraction\n",
    "# des features de style et de contenu, le modèle VGG19 doit\n",
    "# travailler sur des images normalisées.\n",
    "#\n",
    "# As the VGG19 network has been pre-trained on ImageNet\n",
    "# with specific normalization parameters for this dataset, \n",
    "# it is assumed that it will perform better on a new\n",
    "# image distribution if it shares this normalization. Thus, for the extraction\n",
    "# of style and content features, the VGG19 model must\n",
    "# work on normalized images.\n",
    "#\n",
    "# Toutefois, afin d'afficher l'image hybride, il est\n",
    "# important de retirer la normalisation si on \n",
    "# souhaite avoir un résultat visuellement intéressant,\n",
    "# car la normalisation a un impact sur la distribution\n",
    "# des valeurs de pixels dans l'image. La classe suivante\n",
    "# doit vous permettre d'appliquer l'inverse de la\n",
    "# normalisation.\n",
    "#\n",
    "# However, in order to display the hybrid image, it is\n",
    "# important to remove the normalization \n",
    "# to get a visually interesting result,\n",
    "# because normalization has an impact on the distribution\n",
    "# of pixel values in the image. The following class\n",
    "# should allow you to apply the inverse of the\n",
    "# normalization.\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor image of shape (C,H,W).\n",
    "        Returns:\n",
    "            new_x (Tensor): DeNormalized tensor image (C,H,W).\n",
    "        \"\"\"\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom\n",
    "        # pour appliquer l'inverse de la normalisation.\n",
    "        # Vous devez implémenter cette fonction manuellement\n",
    "        # en utilisant des opérations sur les tenseurs.\n",
    "        #\n",
    "        # Implementation of a custom transformation \n",
    "        # to apply the inverse normalization.\n",
    "        # You must create this function manually using\n",
    "        # tensor operations.\n",
    "        return new_x # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "\n",
    "# Pour que les images s'affichent, la valeur des pixels doit\n",
    "# être retournée entre [0,1]. La classe suivante doit vous permettre de borner\n",
    "# les valeurs des pixels de l'image hybride entre [0,1].\n",
    "#\n",
    "# For the images to be displayed, the pixel value must be between [0,1].\n",
    "# The following class should allow you to bound\n",
    "# the pixel values of the hybrid image between [0,1].\n",
    "class Clamp(object):\n",
    "    def __init__(self, min, max):\n",
    "        self.min = float(min)\n",
    "        self.max = float(max)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor of the image.\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor with values clipped within [0,1].\n",
    "        \"\"\"\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation maison\n",
    "        # pour borner les valeurs dans la plage [0, 1]\n",
    "        # Utilisez la fonction PyTorch: Clamp()\n",
    "        #\n",
    "        # Implementation of a custom transformation\n",
    "        # to clamp the values in the range [0, 1].\n",
    "        # Use the PyTorch: Clamp() function\n",
    "        return new_x # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "# Pour que la librairie Matplotlib puisse afficher le\n",
    "# contenu des images, les canaux doivent être\n",
    "# donnés dans le bon ordre. PyTorch utilise les images sous\n",
    "# la forme (B,C,H,W) et Matplotlib doit recevoir les images sous \n",
    "# la forme (H,W,C). Comme le Permute est appelé après le\n",
    "# RemoveDimension(), vous aurez ici, en entrée, un tenseur\n",
    "# (C,H,W) que vous devez transformer dans la forme\n",
    "# désirée pour l'affichage de Matplotlib.\n",
    "#\n",
    "# In order for the Matplotlib library to display the\n",
    "# content of the images, the channels must\n",
    "# be given in the right order. PyTorch uses images in the form\n",
    "# (B,C,H,W) and Matplotlib must receive the images under \n",
    "# the form (H,W,C). As the Permute is called after the\n",
    "# RemoveDimension(), you will have here, as input, a tensor\n",
    "# (C,H,W) that you must transform into the\n",
    "# desired form for the display of Matplotlib.\n",
    "class Permute(object):\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor of the image.\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor of the image with permuted dimensions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom pour\n",
    "        # faire une permutation.\n",
    "        # Utilisez la fonction PyTorch: Permute()\n",
    "        #\n",
    "        # Implementation of a custom permute tranformation.\n",
    "        # Use the function Pytorch: Permute()\n",
    "        return new_x\n",
    "        # ******\n",
    "\n",
    "\n",
    "# *** TODO ***\n",
    "# transforms.Compose applique séquentiellement les\n",
    "# transformations. Étapes de post-traitement (voir les modules précédents)\n",
    "# 1. Retirer la 1ère dimension (B,C,H,W)->(C,H,W)\n",
    "# 2. Appliquer l'inverse de la normalisation ImageNet\n",
    "# 3. Permuter les dimension pour Matplotlib (C,H,W)->(H,W,C)\n",
    "# 4. Clamp les valeurs des tenseurs entre [0,1]\n",
    "# Vous devez ici passer les paramètres désirés dans l'appel des classes \n",
    "# de transformation.\n",
    "#\n",
    "# transforms.Compose applies sequentially the\n",
    "# transforms. Postprocessing steps (see previous modules)\n",
    "# 1. Remove the 1st dimension (B,C,H,W)->(C,H,W)\n",
    "# 2. Apply the inverse of the ImageNet normalization\n",
    "# 3. Swap dimensions for Matplotlib (C,H,W)->(H,W,C)\n",
    "# 4. Clamp the tensor values between [0,1]\n",
    "# Here you have to pass the desired parameters in the call of \n",
    "# transformation classes.\n",
    "postprocessing = transforms.Compose([RemoveDimension(), \n",
    "                                     DeNormalize(mean=IMAGENET_MEAN,\n",
    "                                                 std=IMAGENET_STD),\n",
    "                                     Permute(),\n",
    "                                     Clamp(),\n",
    "                                    ])\n",
    "# ******\n",
    "\n",
    "# Le code suivant est donné pour permettre d'afficher\n",
    "# certaines informations utiles qui vous permettront\n",
    "# de comprendre si vos transformations post-traitement\n",
    "# sont fonctionnelles.\n",
    "#\n",
    "# The following code is given to allow you to display\n",
    "# some useful information that will allow you to\n",
    "# understand if your postprocessing transformations\n",
    "# are functional.\n",
    "\n",
    "# Afficher les statistiques des images naturelles\n",
    "# Display the statistics of natural images\n",
    "for name, img in images.items():\n",
    "    results['Name'].append(f'raw_{name}')\n",
    "    results['Shape'].append(img.size)\n",
    "    mean = numpy.mean(img)/255\n",
    "    results['Mean'].append(mean)\n",
    "    std = numpy.std(img)/255\n",
    "    results['Std'].append(std)\n",
    "\n",
    "# Appliquer le prétraitement sur les images\n",
    "# Apply preprocessing on the images\n",
    "pre_images = {}\n",
    "for k,v in images.items():\n",
    "    pre_images[k] = preprocessing(v)\n",
    "    pre_images[k] = pre_images[k].to(DEVICE)\n",
    "\n",
    "# Afficher les statistiques des images transformées\n",
    "# Display the statistics of the transformed images\n",
    "for name, img in pre_images.items():\n",
    "    results['Name'].append(f'pre_{name}')\n",
    "    results['Shape'].append(img.shape)\n",
    "    results['Mean'].append(img.mean().item())\n",
    "    results['Std'].append(img.std().item())\n",
    "\n",
    "post_images = {}\n",
    "\n",
    "# Appliquer le post-traitement sur les images\n",
    "# Apply postprocessing to images\n",
    "for name,img in pre_images.items():\n",
    "    image = img.cpu().detach()\n",
    "    post_images[name] = postprocessing(image)\n",
    "\n",
    "# Afficher les statistiques des images transformées\n",
    "# Display the statistics of the transformed images\n",
    "for name, img in post_images.items():\n",
    "    results['Name'].append(f'post_{name}')\n",
    "    results['Shape'].append(img.shape)\n",
    "    results['Mean'].append(img.mean().item())\n",
    "    results['Std'].append(img.std().item())\n",
    "\n",
    "# Affichage des résultats\n",
    "# N.B. Bien que la taille de l'image ait été changée par le resize,\n",
    "#      les valeurs de moyenne et de déviation standard devraient être\n",
    "#      très proches avant le prétraitement et après le post-traitement.\n",
    "# Displaying the results\n",
    "# N.B. Although the size of the image has been changed by the resize,\n",
    "# the mean and standard deviation values should be very close\n",
    "# before preprocessing and after postprocessing.\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0ab9f",
   "metadata": {
    "editable": false,
    "id": "95d36f09",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3B dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36bd239",
   "metadata": {
    "editable": false,
    "id": "e822941e",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3B in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2fc52",
   "metadata": {
    "deletable": false,
    "id": "351f306e",
    "tags": [
     "user-answer-D4Q3B",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Classe de transformation Custom pour ajouter un channel à la position \"dim\".\n",
    "# Cette classe vous est donnée comme exemple pour l'implémentation des autres transformations.\n",
    "# Custom transformation class to add a channel at the \"dim\" position.\n",
    "# This class is given as an example for the implementation of other transformations.\n",
    "class AddDimension(object):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C,H,W).\n",
    "\n",
    "        Returns:\n",
    "            tensor (Tensor): Tensor image with an added channel, now of size (1,C,H,W).\n",
    "        \"\"\"\n",
    "\n",
    "        new_x = x.unsqueeze(self.dim)\n",
    "        return new_x\n",
    "    \n",
    "# Étapes de prétraitement\n",
    "# 1. Redimensionner l'images à la taille désirée -> (3, 256, 256)\n",
    "# 2. Transformer l'image PIL en tenseur\n",
    "# 3. Appliquer la normalisation ImageNet\n",
    "# 4. Ajouter une dimension pour PyTorch (C,H,W) -> (B,C,H,W)\n",
    "#    où B est la taille de batch (lot).\n",
    "#\n",
    "# Preprocessing steps\n",
    "# Resize the image to the desired size -> (3, 256, 256)\n",
    "# 2. Transform the PIL image into a tensor\n",
    "# 3. Apply ImageNet normalization\n",
    "# 4. Add a dimension for PyTorch (C,H,W) -> (B,C,H,W)\n",
    "# where B is the batch size.\n",
    "preprocessing = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean=IMAGENET_MEAN,\n",
    "                                                         std=IMAGENET_STD),\n",
    "                                    AddDimension(0),\n",
    "                                   ])\n",
    "\n",
    "# Puisque PyTorch travaille sur des lots (batch)\n",
    "# d'images, une dimension supplémentaire (B) est\n",
    "# ajoutée pour son fonctionnement. L'image en entrée\n",
    "# passe donc de la taille (3, 256, 256) à la taille\n",
    "# (B, 3, 256, 256) où B, ici, est de taille 1, car il\n",
    "# n'y a qu'une seule image par lot.\n",
    "# \n",
    "# Since PyTorch works on batches of images,\n",
    "# an extra dimension (B) is added for its operation. The input\n",
    "# image goes from the size (3, 256, 256) to the size\n",
    "# (B, 3, 256, 256) where B, here, is of size 1, because\n",
    "# there is only one image per batch.\n",
    "#\n",
    "# Toutefois, afin d'afficher l'image hybride, il est important\n",
    "# de retirer cette dimension supplémentaire, car les outils\n",
    "# d'affichage s'attendent à afficher une image unique\n",
    "# Cette prochaine classe doit donc vous permettre de\n",
    "# retirer cette dimension supplémentaire. \n",
    "#\n",
    "# However, in order to display the hybrid image, it is important\n",
    "# to remove this extra dimension, because the display tools\n",
    "# expect to display a single image. \n",
    "# This next class should allow you to \n",
    "# remove this extra dimension. \n",
    "class RemoveDimension(object):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor image of size (1,C,H,W).\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor image with the removed channel, now of size (C,H,W).\n",
    "        \"\"\"\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom\n",
    "        # pour retirer un channel à la position \"dim\"\n",
    "        # Utilisez la fonction Pytorch Squeeze()\n",
    "        # Implementation of a Custom transformation\n",
    "        # to remove a channel at the \"dim\" position\n",
    "        # Use the Pytorch Squeeze() function\n",
    "        new_x = x.squeeze(self.dim)\n",
    "        return new_x  # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "# Comme le réseau VGG19 a été pré-entraîné sur ImageNet\n",
    "# avec des paramètres de normalisation spécifique à ce\n",
    "# jeu de données, on assume qu'il sera plus performant sur\n",
    "# une nouvelle distribution d'images si celle-ci partage\n",
    "# également cette normalisation. Ainsi, pour l'extraction\n",
    "# des features de style et de contenu, le modèle VGG19 doit\n",
    "# travailler sur des images normalisées.\n",
    "#\n",
    "# As the VGG19 network has been pre-trained on ImageNet\n",
    "# with specific normalization parameters for this dataset, \n",
    "# it is assumed that it will perform better on a new\n",
    "# image distribution if it shares this normalization. Thus, for the extraction\n",
    "# of style and content features, the VGG19 model must\n",
    "# work on normalized images.\n",
    "#\n",
    "# Toutefois, afin d'afficher l'image hybride, il est\n",
    "# important de retirer la normalisation si on \n",
    "# souhaite avoir un résultat visuellement intéressant,\n",
    "# car la normalisation a un impact sur la distribution\n",
    "# des valeurs de pixels dans l'image. La classe suivante\n",
    "# doit vous permettre d'appliquer l'inverse de la\n",
    "# normalisation.\n",
    "#\n",
    "# However, in order to display the hybrid image, it is\n",
    "# important to remove the normalization \n",
    "# to get a visually interesting result,\n",
    "# because normalization has an impact on the distribution\n",
    "# of pixel values in the image. The following class\n",
    "# should allow you to apply the inverse of the\n",
    "# normalization.\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor image of shape (C,H,W).\n",
    "        Returns:\n",
    "            new_x (Tensor): DeNormalized tensor image (C,H,W).\n",
    "        \"\"\"\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom\n",
    "        # pour appliquer l'inverse de la normalisation.\n",
    "        # Vous devez implémenter cette fonction manuellement\n",
    "        # en utilisant des opérations sur les tenseurs.\n",
    "        #\n",
    "        # Implementation of a custom transformation \n",
    "        # to apply the inverse normalization.\n",
    "        # You must create this function manually using\n",
    "        # tensor operations.\n",
    "        mean = numpy.array(self.mean)\n",
    "        std = numpy.array(self.std)\n",
    "        u = transforms.Normalize(mean=-mean/std, std=1/std)\n",
    "        new_x = u(x)\n",
    "        return new_x # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "\n",
    "# Pour que les images s'affichent, la valeur des pixels doit\n",
    "# être retournée entre [0,1]. La classe suivante doit vous permettre de borner\n",
    "# les valeurs des pixels de l'image hybride entre [0,1].\n",
    "#\n",
    "# For the images to be displayed, the pixel value must be between [0,1].\n",
    "# The following class should allow you to bound\n",
    "# the pixel values of the hybrid image between [0,1].\n",
    "class Clamp(object):\n",
    "    def __init__(self, min, max):\n",
    "        self.min = float(min)\n",
    "        self.max = float(max)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor of the image.\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor with values clipped within [0,1].\n",
    "        \"\"\"\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation maison\n",
    "        # pour borner les valeurs dans la plage [0, 1]\n",
    "        # Utilisez la fonction PyTorch: Clamp()\n",
    "        #\n",
    "        # Implementation of a custom transformation\n",
    "        # to clamp the values in the range [0, 1].\n",
    "        # Use the PyTorch: Clamp() function\n",
    "        new_x = torch.clamp(x, min=self.min, max=self.max)\n",
    "        return new_x # Retourne le x transformé / return the transformed x\n",
    "        # ******\n",
    "\n",
    "# Pour que la librairie Matplotlib puisse afficher le\n",
    "# contenu des images, les canaux doivent être\n",
    "# donnés dans le bon ordre. PyTorch utilise les images sous\n",
    "# la forme (B,C,H,W) et Matplotlib doit recevoir les images sous \n",
    "# la forme (H,W,C). Comme le Permute est appelé après le\n",
    "# RemoveDimension(), vous aurez ici, en entrée, un tenseur\n",
    "# (C,H,W) que vous devez transformer dans la forme\n",
    "# désirée pour l'affichage de Matplotlib.\n",
    "#\n",
    "# In order for the Matplotlib library to display the\n",
    "# content of the images, the channels must\n",
    "# be given in the right order. PyTorch uses images in the form\n",
    "# (B,C,H,W) and Matplotlib must receive the images under \n",
    "# the form (H,W,C). As the Permute is called after the\n",
    "# RemoveDimension(), you will have here, as input, a tensor\n",
    "# (C,H,W) that you must transform into the\n",
    "# desired form for the display of Matplotlib.\n",
    "class Permute(object):\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor of the image.\n",
    "\n",
    "        Returns:\n",
    "            new_x (Tensor): Tensor of the image with permuted dimensions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentation d'une transformation Custom pour\n",
    "        # faire une permutation.\n",
    "        # Utilisez la fonction PyTorch: Permute()\n",
    "        #\n",
    "        # Implementation of a custom permute tranformation.\n",
    "        # Use the function Pytorch: Permute()\n",
    "        new_x = x.permute(1, 2, 0)\n",
    "        return new_x\n",
    "        # ******\n",
    "\n",
    "\n",
    "# *** TODO ***\n",
    "# transforms.Compose applique séquentiellement les\n",
    "# transformations. Étapes de post-traitement (voir les modules précédents)\n",
    "# 1. Retirer la 1ère dimension (B,C,H,W)->(C,H,W)\n",
    "# 2. Appliquer l'inverse de la normalisation ImageNet\n",
    "# 3. Permuter les dimension pour Matplotlib (C,H,W)->(H,W,C)\n",
    "# 4. Clamp les valeurs des tenseurs entre [0,1]\n",
    "# Vous devez ici passer les paramètres désirés dans l'appel des classes \n",
    "# de transformation.\n",
    "#\n",
    "# transforms.Compose applies sequentially the\n",
    "# transforms. Postprocessing steps (see previous modules)\n",
    "# 1. Remove the 1st dimension (B,C,H,W)->(C,H,W)\n",
    "# 2. Apply the inverse of the ImageNet normalization\n",
    "# 3. Swap dimensions for Matplotlib (C,H,W)->(H,W,C)\n",
    "# 4. Clamp the tensor values between [0,1]\n",
    "# Here you have to pass the desired parameters in the call of \n",
    "# transformation classes.\n",
    "postprocessing = transforms.Compose([RemoveDimension(0), \n",
    "                                     DeNormalize(mean=IMAGENET_MEAN,\n",
    "                                                 std=IMAGENET_STD),\n",
    "                                     Permute(0),\n",
    "                                     Clamp(0, 1),\n",
    "                                    ])\n",
    "# ******\n",
    "\n",
    "# Le code suivant est donné pour permettre d'afficher\n",
    "# certaines informations utiles qui vous permettront\n",
    "# de comprendre si vos transformations post-traitement\n",
    "# sont fonctionnelles.\n",
    "#\n",
    "# The following code is given to allow you to display\n",
    "# some useful information that will allow you to\n",
    "# understand if your postprocessing transformations\n",
    "# are functional.\n",
    "\n",
    "# Afficher les statistiques des images naturelles\n",
    "# Display the statistics of natural images\n",
    "for name, img in images.items():\n",
    "    results['Name'].append(f'raw_{name}')\n",
    "    results['Shape'].append(img.size)\n",
    "    mean = numpy.mean(img)/255\n",
    "    results['Mean'].append(mean)\n",
    "    std = numpy.std(img)/255\n",
    "    results['Std'].append(std)\n",
    "\n",
    "# Appliquer le prétraitement sur les images\n",
    "# Apply preprocessing on the images\n",
    "pre_images = {}\n",
    "for k,v in images.items():\n",
    "    pre_images[k] = preprocessing(v)\n",
    "    pre_images[k] = pre_images[k].to(DEVICE)\n",
    "\n",
    "# Afficher les statistiques des images transformées\n",
    "# Display the statistics of the transformed images\n",
    "for name, img in pre_images.items():\n",
    "    results['Name'].append(f'pre_{name}')\n",
    "    results['Shape'].append(img.shape)\n",
    "    results['Mean'].append(img.mean().item())\n",
    "    results['Std'].append(img.std().item())\n",
    "\n",
    "post_images = {}\n",
    "\n",
    "# Appliquer le post-traitement sur les images\n",
    "# Apply postprocessing to images\n",
    "for name,img in pre_images.items():\n",
    "    image = img.cpu().detach()\n",
    "    post_images[name] = postprocessing(image)\n",
    "\n",
    "# Afficher les statistiques des images transformées\n",
    "# Display the statistics of the transformed images\n",
    "for name, img in post_images.items():\n",
    "    results['Name'].append(f'post_{name}')\n",
    "    results['Shape'].append(img.shape)\n",
    "    results['Mean'].append(img.mean().item())\n",
    "    results['Std'].append(img.std().item())\n",
    "\n",
    "# Affichage des résultats\n",
    "# N.B. Bien que la taille de l'image ait été changée par le resize,\n",
    "#      les valeurs de moyenne et de déviation standard devraient être\n",
    "#      très proches avant le prétraitement et après le post-traitement.\n",
    "# Displaying the results\n",
    "# N.B. Although the size of the image has been changed by the resize,\n",
    "# the mean and standard deviation values should be very close\n",
    "# before preprocessing and after postprocessing.\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524a26f",
   "metadata": {
    "editable": false,
    "id": "89bf85d3",
    "lang": "fr"
   },
   "source": [
    "## Q3C\n",
    "Maintenant que nous avons les données pour l'entraînement et qu'elles sont formatées et normalisées, il faut télécharger le modèle préentraîné VGG19 de la librairie PyTorch. Puisque nous n'avons pas besoin des couches de classification, seules les couches de *features* sont stockées dans la variable `vgg`. Pour ce faire, affichez les noms des modules et couches du modèle et ne sélectionnez que ce qui correspond aux couches de *features*. Vous ne voulez pas conserver les couches de classification, car elles ne sont pas utiles pour la suite du problème. Ensuite, vous devez geler les paramètres du réseau, car ils ne seront pas modifiés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175426e2",
   "metadata": {
    "editable": false,
    "id": "a1e83145",
    "lang": "en"
   },
   "source": [
    "## Q3C\n",
    "Now that we have the data for training and it is formatted and normalized, we need to download the VGG19 pre-trained model from the PyTorch library. Since we don't need the classification layers, we only store the *features* layers in the variable `vgg`. To do this, display the names of the modules and layers in the model and select only those that correspond to the *features* layers. You don't want to keep the classification layers, as they are not useful for the rest of the problem. Next, you need to freeze the network settings, as they will not be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c413b",
   "metadata": {
    "editable": false,
    "id": "6f549f85",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e7ba1",
   "metadata": {
    "editable": false,
    "id": "45c77bd5",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3C answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a7866",
   "metadata": {
    "editable": false,
    "id": "1f8da436"
   },
   "outputs": [],
   "source": [
    "# *** TODO ***\n",
    "# Télécharger la portion \"features\" du VGG19\n",
    "# Nous n'avons pas besoin des couches de classification.\n",
    "# !!! Veuillez passer en paramètre progress=False dans la  !!!\n",
    "# !!! fonction. Autrement, l'exécution vous retournera une !!!\n",
    "# !!! erreur.                                              !!!\n",
    "# Geler les couches pré-entraînées\n",
    "\n",
    "# Download the features portion of the VGG19\n",
    "# We don't need the classification layers.\n",
    "# !!! Please pass in the progress=False parameter in the !!!\n",
    "# !!! function. Otherwise, the execution will return an !!!\n",
    "# !!! error. !!!\n",
    "# Freeze pre-trained layers\n",
    "# ******\n",
    "\n",
    "# Si GPU disponible, monter le modèle sur le GPU\n",
    "# If GPU available, mount the model on the GPU\n",
    "vgg.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3b0e1",
   "metadata": {
    "editable": false,
    "id": "c76ddbf4",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3C dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63570092",
   "metadata": {
    "editable": false,
    "id": "3a2648d6",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3C in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e6123",
   "metadata": {
    "deletable": false,
    "id": "d66b8513",
    "tags": [
     "user-answer-D4Q3C",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# *** TODO ***\n",
    "# Télécharger la portion \"features\" du VGG19\n",
    "# Nous n'avons pas besoin des couches de classification.\n",
    "# !!! Veuillez passer en paramètre progress=False dans la  !!!\n",
    "# !!! fonction. Autrement, l'exécution vous retournera une !!!\n",
    "# !!! erreur.                                              !!!\n",
    "# Geler les couches pré-entraînées\n",
    "#\n",
    "# Download the features portion of the VGG19\n",
    "# We don't need the classification layers.\n",
    "# !!! Please pass in the progress=False parameter in the !!!\n",
    "# !!! function. Otherwise, the execution will return an !!!\n",
    "# !!! error. !!!\n",
    "# Freeze pre-trained layers\n",
    "# ******\n",
    "vgg = models.vgg19(pretrained=True, progress=False).features\n",
    "for name, param in vgg.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Si GPU disponible, monter le modèle sur le GPU\n",
    "# If GPU available, mount the model on the GPU\n",
    "vgg.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b8393b",
   "metadata": {
    "editable": false,
    "id": "ad6d6e30",
    "lang": "fr"
   },
   "source": [
    "## Q3D\n",
    "Afin de pouvoir développer les fonctions de perte (*loss*) et lancer la génération de l'image hybride, il faut tout d'abord initialiser les pondérations et calculer quelques paramètres importants. Vous devez donc:\n",
    "1. Extraire les *features* de l'image de style avec la fonction `extract_features`;\n",
    "2. Extraire les *features* de l'image de contenu avec la fonction `extract_features`;\n",
    "3. Créer une copie de l'image de contenu (appelée image cible) pour permettre de l'ajuster itérativement tout en gardant une copie du contenu original.\n",
    "\n",
    "Également, vous êtes invités à ajuster les pondérations et les paramètres $\\alpha$ et $\\beta$ pour rendre votre rendu le plus à votre goût possible. Notez bien que vous n'êtes pas évalué sur la qualité du rendu, mais on vous encourage fortement à tester d'autres configurations de paramètres pour bien en comprendre le fonctionnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23851f58",
   "metadata": {
    "editable": false,
    "id": "9ae3b6ee",
    "lang": "en"
   },
   "source": [
    "## Q3D\n",
    "In order to develop the loss functions and launch the generation of the hybrid image, you must first initialize the weights and calculate some important parameters. So you need to:\n",
    "1. Extract the *features* from the style image with the `extract_features` function;\n",
    "2. Extract the *features* from the content image with the `extract_features` function;\n",
    "3. Create a copy of the content image (called the target image) to allow it to be adjusted iteratively while keeping a copy of the original content.\n",
    "\n",
    "Also, you are invited to adjust the weights and the $\\alpha$ and $\\beta$ parameters to make your rendering as pleasing as possible. Note that you are not evaluated on the quality of the rendering, but you are strongly encouraged to test other parameter configurations to understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca73ab",
   "metadata": {
    "editable": false,
    "id": "6b589033",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67308a0e",
   "metadata": {
    "editable": false,
    "id": "3cc649cf",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3D answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b12d68",
   "metadata": {
    "editable": false,
    "id": "b62199ef"
   },
   "outputs": [],
   "source": [
    "# ** TODO ***\n",
    "# Extraire les features de l'image de style avec la fonction\n",
    "# extract_features.\n",
    "# Extract the features of the style image with the function\n",
    "# extract_features.\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Extraire les features de l'image de content avec la fonction\n",
    "# extract_features.\n",
    "# Extract the features of the content image with the function\n",
    "# extract_features.\n",
    "# ******\n",
    "\n",
    "# Pré-calculer la matrice de Gram pour chaque couche de style\n",
    "# Pre-compute the Gram matrix for each style layer\n",
    "style_grams = {}\n",
    "for layer in style_features:\n",
    "    style_grams[layer] = gram_matrix(style_features[layer])\n",
    "\n",
    "# *** TODO ***\n",
    "# Création d'une image cible temporaire. Utilisez la fonction clone() de\n",
    "# la librairie PyTorch. N'oubliez pas le gradient! Considérez également le\n",
    "# device utilisé (CPU vs GPU). Il faut travailler sur une copie de\n",
    "# l'image cible pour changer son style itérativement.\n",
    "#\n",
    "# Create a temporary target image. Use the clone() function of\n",
    "# the PyTorch library. Don't forget the gradient! Also consider the\n",
    "# device used (CPU vs GPU). You have to work on a copy of the\n",
    "# the target image to change its style iteratively.\n",
    "# ******\n",
    "\n",
    "# Poids appliqués pour chaque couche de style\n",
    "# Weights applied for each style layer \n",
    "# Valeurs par défaut / default values:\n",
    "# 'conv1_1': 1.\n",
    "# 'conv2_1': 0.75\n",
    "# 'conv3_1': 0.2\n",
    "# 'conv4_1': 0.2\n",
    "# 'conv5_1': 0.2\n",
    "style_layers_weights = {'conv1_1': 1.,\n",
    "                        'conv2_1': 0.75,\n",
    "                        'conv3_1': 0.2,\n",
    "                        'conv4_1': 0.2,\n",
    "                        'conv5_1': 0.2}\n",
    "\n",
    "# Par défaut: content_weight = 1\n",
    "# By default: content_weight = 1\n",
    "content_weight = 1 \n",
    "\n",
    "# Par défaut: style_weight = 1e7\n",
    "# By default: style_weight = 1e7\n",
    "style_weight = 1e7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b98083",
   "metadata": {
    "editable": false,
    "id": "a3202af2",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3D dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739b1ee",
   "metadata": {
    "editable": false,
    "id": "6bbcc775",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3D in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12152d6",
   "metadata": {
    "deletable": false,
    "id": "5658d3ca",
    "tags": [
     "user-answer-D4Q3D",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# ** TODO ***\n",
    "# Extraire les features de l'image de style avec la fonction\n",
    "# extract_features.\n",
    "# Extract the features of the style image with the function\n",
    "# extract_features.\n",
    "# ******\n",
    "style_features = extract_features(pre_images[STYLE_IMAGE], vgg)\n",
    "# *** TODO ***\n",
    "# Extraire les features de l'image de content avec la fonction\n",
    "# extract_features.\n",
    "# Extract the features of the content image with the function\n",
    "# extract_features.\n",
    "# ******\n",
    "content_features = extract_features(pre_images[CONTENT_IMAGE], vgg)\n",
    "\n",
    "# Pré-calculer la matrice de Gram pour chaque couche de style\n",
    "# Pre-compute the Gram matrix for each style layer\n",
    "style_grams = {}\n",
    "for layer in style_features:\n",
    "    style_grams[layer] = gram_matrix(style_features[layer])\n",
    "\n",
    "# *** TODO ***\n",
    "# Création d'une image cible temporaire. Utilisez la fonction clone() de\n",
    "# la librairie PyTorch. N'oubliez pas le gradient! Considérez également le\n",
    "# device utilisé (CPU vs GPU). Il faut travailler sur une copie de\n",
    "# l'image cible pour changer son style itérativement.\n",
    "#\n",
    "# Create a temporary target image. Use the clone() function of\n",
    "# the PyTorch library. Don't forget the gradient! Also consider the\n",
    "# device used (CPU vs GPU). You have to work on a copy of the\n",
    "# the target image to change its style iteratively.\n",
    "# ******\n",
    "target = pre_images[CONTENT_IMAGE].clone()\n",
    "target.requires_grad = True\n",
    "\n",
    "# Poids appliqués pour chaque couche de style\n",
    "# Weights applied for each style layer \n",
    "# Valeurs par défaut / default values:\n",
    "# 'conv1_1': 1.\n",
    "# 'conv2_1': 0.75\n",
    "# 'conv3_1': 0.2\n",
    "# 'conv4_1': 0.2\n",
    "# 'conv5_1': 0.2\n",
    "style_layers_weights = {'conv1_1': 1.,\n",
    "                        'conv2_1': 0.75,\n",
    "                        'conv3_1': 0.2,\n",
    "                        'conv4_1': 0.2,\n",
    "                        'conv5_1': 0.2}\n",
    "\n",
    "# Par défaut: content_weight = 1\n",
    "# By default: content_weight = 1\n",
    "content_weight = 1 \n",
    "\n",
    "# Par défaut: style_weight = 1e7\n",
    "# By default: style_weight = 1e7\n",
    "style_weight = 1e7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d1fcb",
   "metadata": {
    "editable": false,
    "id": "2dc6b8dc",
    "lang": "fr"
   },
   "source": [
    "## Q3E\n",
    "Le transfert de style, comme vous pouvez le comprendre dans l'article de [Gatys et coll.](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), ne requière pas d'entraînement à proprement parler. En effet, puisqu'on utilise un réseau préentraîné et qu'on en gèle les poids, le réseau n'apprend rien. Toutefois, les pixels de l'image cible (hybride) doivent être optimisés pour contenir à la fois le *style* ainsi que le *contenu* désiré. On fait donc une descente de gradient avec fonction de perte pour minimiser la différence de contenu entre l'image originale et l'image générée, tout en intégrant le nouveau style à l'image générée.\n",
    "\n",
    "Pour se faire, deux fonctions de pertes doivent être développées: une qui permet de mesurer la différence de contenu entre l'image cible et l'image de contenu (*perte de contenu*) ainsi qu'une seconde qui permet de mesurer la différence de style entre la matrice Gram de l'image cible et celle de l'image de style (*perte de style*). La somme pondérée de ces deux fonctions permet de générer la *perte totale* qui sera utilisée pour l'optimisation des pixels de l'image hybride.\n",
    "\n",
    "1. Implémenter la fonction `calculate_content_loss` qui prend en paramètre le nom de la couche $l$ à évaluer et qui retourne la perte de contenu entre les paramètres de l'image cible ($F$) et les paramètres de l'image de contenu ($P$). La fonction de calcul de la perte est la suivante:\n",
    "$$L_{content}(\\vec{p},\\vec{x},l)=\\frac{1}{2}\\sum_{i,j}(F_{i,j}^l-P_{i,j}^l)^2,$$ où la couche qui doit être utilisée pour calculer la perte de contenu est la 21e couche du modèle VGG19. Vous retrouverez le nom de cette couche dans le graphe du modèle.\n",
    "\n",
    "2. Implémenter la fonction `calculate_style_loss` qui retourne la perte de style entre la matrice Gram de l'image cible ($G$) et la matrice Gram de l'image de style ($A$). La fonction de calcul de cette perte est: $$E_l=\\frac{1}{4N_l^2M_l^2} \\sum_{i,j}(G_{i,j}^l-A_{i,j}^l)^2,$$ où $M$ et $N$ sont des paramètres que vous devez extraire de votre compréhension de l'article, suivi de: $$L_{style}(\\vec{a},\\vec{x})=\\sum_{l=0}^{L}w_{l}E_{l},$$ où $w_l$ est la pondération donnée à la couche $l$.\n",
    "\n",
    "3. Implémenter la fonction `calculate_total_loss` qui retourne la perte totale pour l'itération actuelle. La fonction pour vous permettre de calculer cette perte est: $$L_{total}(\\vec{a},\\vec{p},\\vec{x})=\\alpha L_{content}(\\vec{p},\\vec{x})+\\beta L_{style}(\\vec{a},\\vec{x})$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c3e82",
   "metadata": {
    "editable": false,
    "id": "9fa2b4f5",
    "lang": "en"
   },
   "source": [
    "## Q3E\n",
    "Style transfer, as you can read in the paper by [Gatys et al](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), does not require training per se. Indeed, since we use a pre-trained network and freeze the weights, the network will not learn anything. However, the pixels of the target (hybrid) image must be optimized to contain both the *style* and the desired *content*. We therefore perform a gradient descent with a loss function to minimize the difference in content between the original image and the generated image, while integrating the new style into the generated image.\n",
    "\n",
    "To do this, two loss functions must be developed: one that measures the difference in content between the target image and the content image (*content loss*) and a second that measures the difference in style between the Gram matrix of the target image and the style image (*style loss*). The weighted sum of these two functions allows to generate the *total loss* which will be used for the optimization of the pixels of the hybrid image.\n",
    "\n",
    "1. Implement the function `calculate_content_loss` which takes as parameter the name of the layer $l$ to be evaluated and returns the content loss between the parameters of the target image ($F$) and the parameters of the content image ($P$). The function for computing the loss is as follows:\n",
    "$$L_{content}(\\vec{p},\\vec{x},l)=\\frac{1}{2}\\sum_{i,j}(F_{i,j}^l-P_{i,j}^l)^2$$, where the layer that is to be used to compute the content loss is the 21st layer of the VGG19 model. You will find the name of this layer in the model graph.\n",
    "\n",
    "2. Implement the function `calculate_style_loss` which returns the style loss between the Gram matrix of the target image ($G$) and the Gram matrix of the style image ($A$). The function of calculation of this loss is: $$E_l=frac{1}{4N_l^2M_l^2} \\sum_{i,j}(G_{i,j}^l-A_{i,j}^l)^2,$$ where $M$ and $N$ are parameters you need to extract from your understanding of the article, followed by: $$L_{style}(\\vec{a},\\vec{x})=\\sum_{l=0}^{L}w_{l}E_{l},$$ where $w_l$ is the weight given to the $l$ layer.\n",
    "\n",
    "3. Implement the function `calculate_total_loss` which returns the total loss for the current iteration. The function to allow you to calculate this loss is: $$L_{total}(\\vec{a},\\vec{p},\\vec{x})=$alpha L_{content}(\\vec{p},\\vec{x})+$beta L_{style}(\\vec{a},\\vec{x})$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa8001",
   "metadata": {
    "editable": false,
    "id": "fc015eee"
   },
   "outputs": [],
   "source": [
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte de contenu pour\n",
    "# une couche donnée. La perte est calculée comme l'erreur quadratique\n",
    "# entre les paramètres de l'image cible et les paramètres de l'image\n",
    "# de contenu pour chaque couche.\n",
    "#\n",
    "# Implement the function that calculates the content loss for\n",
    "# a given layer. The loss is calculated as the squared error\n",
    "# between the parameters of the target image and the parameters\n",
    "# of the content image for each layer.\n",
    "def calculate_content_loss(layer_name):\n",
    "    assert(layer_name in target_features.keys())\n",
    "    assert(layer_name in content_features.keys())\n",
    "    \"\"\"\n",
    "    Calculates the content loss between the target image features and\n",
    "    the content image features.\n",
    "\n",
    "    Args:\n",
    "        layer_name (String) : Name of the layer to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tensor (Tensor): Tensor containing the loss of the squared mean\n",
    "                         difference between the target and content layers.\n",
    "    \"\"\"\n",
    "    return content_loss\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte de style pour une couche donnée.\n",
    "# Cette perte de style est calculée par l'erreur entre la matrice Gram\n",
    "# de contenu et la matrice Gram de style, pondérée par le poids donné à chaque couche.\n",
    "#\n",
    "# Implement the function that calculates the style loss for a given layer.\n",
    "# This style loss is calculated as the error between the content Gram\n",
    "# matrix and the style Gram matrix, weighted by the weight given to each layer.\n",
    "def calculate_style_loss(weight_layer, target_gram, style_gram, target_feature):\n",
    "    \"\"\"\n",
    "    Calculates the style loss between the Gram matrix of the image features and\n",
    "    the Gram matrix of the content image features.\n",
    "\n",
    "    Args:\n",
    "        weight_layer (Float) : weighting for the current layer (w_l).\n",
    "        target_gram (Tensor) : Gram matrix of the target image (G).\n",
    "        style_gram (Tensor) : Gram matrix of the style image (A).\n",
    "        target_feature (Tensor) : tensor containing the target feature for\n",
    "                                  the current layer.\n",
    "\n",
    "    Returns:\n",
    "        style_loss (Float): computed style loss for the current layer. \n",
    "    \"\"\"\n",
    "    return style_loss\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte totale pour l'itération.\n",
    "# La perte totale est calculée par la somme pondérée de la perte de contenu\n",
    "# et la perte de style.\n",
    "# Implement the function that calculates the total loss for the iteration.\n",
    "# The total loss is calculated by the weighted sum of the content loss\n",
    "# and the style loss.\n",
    "def calculate_total_loss(content_weight, content_loss, style_weight, style_loss):\n",
    "    \"\"\"\n",
    "    Calculates the total loss for the current iteration.\n",
    "\n",
    "    Args:\n",
    "        content_weight (Float) : Alpha weighting for the content.\n",
    "        content_loss (Float) : Content loss.\n",
    "        style_weight (Float) : Beta weighting for the style.\n",
    "        style_loss (Float) : Total loss.\n",
    "\n",
    "    Returns:\n",
    "        total_loss (Float): computed total loss for the current iteration. \n",
    "    \"\"\"\n",
    "    return total_loss\n",
    "# ******\n",
    "\n",
    "# Nombre total d'itérations pour appliquer\n",
    "# le transfert de style (Min: 2000 | Recommandé: 5000)\n",
    "# Total number of iterations to apply\n",
    "# style transfer (Min: 2000 | Recommended: 5000)\n",
    "steps = 5000\n",
    "\n",
    "# Fréquence de mise à jour de l'image (Valeur recommandée: 500)\n",
    "# Image update frequency (Recommended value: 500)\n",
    "show_image_every = 500\n",
    "\n",
    "# Initialisation de l'optimiseur Adam. Puisqu'on modifie la cible,\n",
    "# on l'applique directement sur les pixels de l'image (learning_rate = 3e-3).\n",
    "# Initialization of the Adam optimizer. Since we modify the target,\n",
    "# we apply it directly on the pixels of the image (learning_rate = 3e-3).\n",
    "#\n",
    "# [Gatys et coll, 2016] font usage de L-BFGS mais pour simplifier l'implémentation et accélérer\n",
    "# la convergence vers des résultats visibles, Adam est plus approprié.\n",
    "# [Gatys et al, 2016] make use of L-BFGS but to simplify implementation and accelerate\n",
    "# convergence to visible results, Adam is more appropriate.\n",
    "optimizer = optim.Adam([target], lr=3e-3)\n",
    "\n",
    "for s in tqdm(range(1, steps+1)):\n",
    "\n",
    "    # 1. Remise à zéro des gradients\n",
    "    # 1. Reset the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. Extraire les features de l'image cible\n",
    "    # 2. Extract the features of the target image\n",
    "    target_features = extract_features(target, vgg)\n",
    "    \n",
    "    # *** TODO ***\n",
    "    # 3. Calculer la loss de contenu avec la fonction\n",
    "    # calculate_content_loss. Vous devez retrouver le\n",
    "    # nom de la couche dans le graphe du modèle.\n",
    "    # 3. Calculate the content loss with the function\n",
    "    # calculate_content_loss. You must find the\n",
    "    # name of the layer in the model graph.\n",
    "    layer_name = \"\"\n",
    "    content_loss = calculate_content_loss(layer_name)\n",
    "    # ******\n",
    "\n",
    "    # 4. Calculer la loss de style en accumulant sa valeur pour chaque couche\n",
    "    # 4. Calculate the style loss by accumulating its value for each layer\n",
    "    style_loss = 0 # Initialiser la loss de style à zéro / Initialise style loss to zero\n",
    "    for l_name, l_weight in style_layers_weights.items():\n",
    "        \n",
    "        # Extraire le contenu de la couche / Extract layer content\n",
    "        target_feature = target_features[l_name]\n",
    "\n",
    "        # Calculer la matrice de Gram du contenu / Compute content Gram matrix\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "\n",
    "        # Extraire la matrice Gram pré-calculée pour le style\n",
    "        # Extract the pre-computed Gram matrix for the style\n",
    "        style_gram = style_grams[l_name]\n",
    "\n",
    "        # Calculer la loss de style avec pondération pour \n",
    "        # la couche donnée avec la fonction calculate_style_loss().\n",
    "        # Calculate the weighted syle loss for \n",
    "        # the given layer with the function calculate_style_loss().\n",
    "        layer_style_loss = calculate_style_loss(l_weight,\n",
    "                                                target_gram,\n",
    "                                                style_gram,\n",
    "                                                target_feature)\n",
    "\n",
    "        # Accumuler la loss de style / Accumulate style loss\n",
    "        style_loss += layer_style_loss\n",
    "    \n",
    "    # 5. Calculer la loss totale avec la fonction calculate_total_loss()\n",
    "    # 5. Calculate the total loss with the function calculate_total_loss()\n",
    "    total_loss = calculate_total_loss(content_weight,\n",
    "                                      content_loss,\n",
    "                                      style_weight,\n",
    "                                      style_loss)\n",
    "    \n",
    "    # 6. Mettre à jour l'image cible\n",
    "    # 6. Update the target image\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Afficher les images intermédiaires\n",
    "    # Display intermediate images\n",
    "    if  s % show_image_every == 0:\n",
    "        # Appliquer le postprocessing sur les images\n",
    "        # Apply postprocessing to the images\n",
    "        plt.figure(figsize=(10,10))\n",
    "        img = target.cpu().detach()\n",
    "        img_post = postprocessing(img)\n",
    "        plt.imshow(img_post)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Libère la cache sur le GPU *important sur un cluster de GPU*\n",
    "# Free the cache on the GPU *important on a GPU cluster*.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09c8fc",
   "metadata": {
    "editable": false,
    "id": "1dcb63af",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3E dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635074d",
   "metadata": {
    "editable": false,
    "id": "43e6e2c6",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3E in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485cd0f",
   "metadata": {
    "deletable": false,
    "id": "690f8a87",
    "tags": [
     "user-answer-D4Q3E",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte de contenu pour\n",
    "# une couche donnée. La perte est calculée comme l'erreur quadratique\n",
    "# entre les paramètres de l'image cible et les paramètres de l'image\n",
    "# de contenu pour chaque couche.\n",
    "#\n",
    "# Implement the function that calculates the content loss for\n",
    "# a given layer. The loss is calculated as the squared error\n",
    "# between the parameters of the target image and the parameters\n",
    "# of the content image for each layer.\n",
    "def calculate_content_loss(layer_name):\n",
    "    assert(layer_name in target_features.keys())\n",
    "    assert(layer_name in content_features.keys())\n",
    "    \"\"\"\n",
    "    Calculates the content loss between the target image features and\n",
    "    the content image features.\n",
    "\n",
    "    Args:\n",
    "        layer_name (String) : Name of the layer to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tensor (Tensor): Tensor containing the loss of the squared mean\n",
    "                         difference between the target and content layers.\n",
    "    \"\"\"    \n",
    "    content_loss = torch.sum((target_features[layer_name] - content_features[layer_name])**2)/2\n",
    "    return content_loss\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte de style pour une couche donnée.\n",
    "# Cette perte de style est calculée par l'erreur entre la matrice Gram\n",
    "# de contenu et la matrice Gram de style, pondérée par le poids donné à chaque couche.\n",
    "#\n",
    "# Implement the function that calculates the style loss for a given layer.\n",
    "# This style loss is calculated as the error between the content Gram\n",
    "# matrix and the style Gram matrix, weighted by the weight given to each layer.\n",
    "def calculate_style_loss(weight_layer, target_gram, style_gram, target_feature):\n",
    "    \"\"\"\n",
    "    Calculates the style loss between the Gram matrix of the image features and\n",
    "    the Gram matrix of the content image features.\n",
    "\n",
    "    Args:\n",
    "        weight_layer (Float) : weighting for the current layer (w_l).\n",
    "        target_gram (Tensor) : Gram matrix of the target image (G).\n",
    "        style_gram (Tensor) : Gram matrix of the style image (A).\n",
    "        target_feature (Tensor) : tensor containing the target feature for\n",
    "                                  the current layer.\n",
    "\n",
    "    Returns:\n",
    "        style_loss (Float): computed style loss for the current layer. \n",
    "    \"\"\"\n",
    "    N_l = target_feature.size(dim=1)\n",
    "    M_l = target_feature.size(dim=2) * target_feature.size(dim=3)\n",
    "    E_l = torch.sum((target_gram - style_gram)**2)/(4*(N_l**2)*(M_l**2))\n",
    "    style_loss = weight_layer*E_l\n",
    "    return style_loss\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Implémenter la fonction qui calcule la perte totale pour l'itération.\n",
    "# La perte totale est calculée par la somme pondérée de la perte de contenu\n",
    "# et la perte de style.\n",
    "# Implement the function that calculates the total loss for the iteration.\n",
    "# The total loss is calculated by the weighted sum of the content loss\n",
    "# and the style loss.\n",
    "def calculate_total_loss(content_weight, content_loss, style_weight, style_loss):\n",
    "    \"\"\"\n",
    "    Calculates the total loss for the current iteration.\n",
    "\n",
    "    Args:\n",
    "        content_weight (Float) : Alpha weighting for the content.\n",
    "        content_loss (Float) : Content loss.\n",
    "        style_weight (Float) : Beta weighting for the style.\n",
    "        style_loss (Float) : Total loss.\n",
    "\n",
    "    Returns:\n",
    "        total_loss (Float): computed total loss for the current iteration. \n",
    "    \"\"\"\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    return total_loss\n",
    "# ******\n",
    "\n",
    "# Nombre total d'itérations pour appliquer\n",
    "# le transfert de style (Min: 2000 | Recommandé: 5000)\n",
    "# Total number of iterations to apply\n",
    "# style transfer (Min: 2000 | Recommended: 5000)\n",
    "steps = 5000\n",
    "\n",
    "# Fréquence de mise à jour de l'image (Valeur recommandée: 500)\n",
    "# Image update frequency (Recommended value: 500)\n",
    "show_image_every = 500\n",
    "\n",
    "# Initialisation de l'optimiseur Adam. Puisqu'on modifie la cible,\n",
    "# on l'applique directement sur les pixels de l'image (learning_rate = 3e-3).\n",
    "# Initialization of the Adam optimizer. Since we modify the target,\n",
    "# we apply it directly on the pixels of the image (learning_rate = 3e-3).\n",
    "#\n",
    "# [Gatys et coll, 2016] font usage de L-BFGS mais pour simplifier l'implémentation et accélérer\n",
    "# la convergence vers des résultats visibles, Adam est plus approprié.\n",
    "# [Gatys et al, 2016] make use of L-BFGS but to simplify implementation and accelerate\n",
    "# convergence to visible results, Adam is more appropriate.\n",
    "optimizer = optim.Adam([target], lr=3e-3)\n",
    "\n",
    "for s in tqdm(range(1, steps+1)):\n",
    "\n",
    "    # 1. Remise à zéro des gradients\n",
    "    # 1. Reset the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. Extraire les features de l'image cible\n",
    "    # 2. Extract the features of the target image\n",
    "    target_features = extract_features(target, vgg)\n",
    "    \n",
    "    # *** TODO ***\n",
    "    # 3. Calculer la loss de contenu avec la fonction\n",
    "    # calculate_content_loss. Vous devez retrouver le\n",
    "    # nom de la couche dans le graphe du modèle.\n",
    "    # 3. Calculate the content loss with the function\n",
    "    # calculate_content_loss. You must find the\n",
    "    # name of the layer in the model graph.\n",
    "    layer_name = \"conv4_2\"\n",
    "    content_loss = calculate_content_loss(layer_name)\n",
    "    # ******\n",
    "\n",
    "    # 4. Calculer la loss de style en accumulant sa valeur pour chaque couche\n",
    "    # 4. Calculate the style loss by accumulating its value for each layer\n",
    "    style_loss = 0 # Initialiser la loss de style à zéro / Initialise style loss to zero\n",
    "    for l_name, l_weight in style_layers_weights.items():\n",
    "        \n",
    "        # Extraire le contenu de la couche / Extract layer content\n",
    "        target_feature = target_features[l_name]\n",
    "\n",
    "        # Calculer la matrice de Gram du contenu / Compute content Gram matrix\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "\n",
    "        # Extraire la matrice Gram pré-calculée pour le style\n",
    "        # Extract the pre-computed Gram matrix for the style\n",
    "        style_gram = style_grams[l_name]\n",
    "\n",
    "        # Calculer la loss de style avec pondération pour \n",
    "        # la couche donnée avec la fonction calculate_style_loss().\n",
    "        # Calculate the weighted syle loss for \n",
    "        # the given layer with the function calculate_style_loss().\n",
    "        layer_style_loss = calculate_style_loss(l_weight,\n",
    "                                                target_gram,\n",
    "                                                style_gram,\n",
    "                                                target_feature)\n",
    "\n",
    "        # Accumuler la loss de style / Accumulate style loss\n",
    "        style_loss += layer_style_loss\n",
    "    \n",
    "    # 5. Calculer la loss totale avec la fonction calculate_total_loss()\n",
    "    # 5. Calculate the total loss with the function calculate_total_loss()\n",
    "    total_loss = calculate_total_loss(content_weight,\n",
    "                                      content_loss,\n",
    "                                      style_weight,\n",
    "                                      style_loss)\n",
    "    \n",
    "    # 6. Mettre à jour l'image cible\n",
    "    # 6. Update the target image\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Afficher les images intermédiaires\n",
    "    # Display intermediate images\n",
    "    if  s % show_image_every == 0:\n",
    "        # Appliquer le postprocessing sur les images\n",
    "        # Apply postprocessing to the images\n",
    "        plt.figure(figsize=(10,10))\n",
    "        img = target.cpu().detach()\n",
    "        img_post = postprocessing(img)\n",
    "        plt.imshow(img_post)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Libère la cache sur le GPU *important sur un cluster de GPU*\n",
    "# Free the cache on the GPU *important on a GPU cluster*.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4205d0",
   "metadata": {
    "editable": false,
    "id": "5cdd35fc",
    "lang": "fr"
   },
   "source": [
    "## Q3F\n",
    "Pour ce cas du transfert de style, plusieurs paramètres peuvent être modifiés pour permettre de modifier le rendu de l'image hybride. La pondération pour chacune des couches de style peut être modifiée ainsi que les paramètres $\\alpha$ et $\\beta$. En fonction de votre compréhension de l'article et de vos tests, observez l'effet de chacun de ces paramètres.\n",
    "\n",
    "Dans la cellule de réponse prévue à cet effet, veuillez répondre aux questions suivantes:\n",
    "1. Quel est l'effet de pondérer différemment les couches sur le style?\n",
    "2. Que se passe-t-il si on pondère plus fortement les premières couches?\n",
    "3. Que se passe-t-il si on pondère plus fortement les dernières couches?\n",
    "4. Quel est l'effet du paramètre $\\alpha$?\n",
    "5. Quel est l'effet du paramètre $\\beta$?\n",
    "6. Qu'est-ce qui est, selon vous, une bonne configuration des paramètres de pondération des poids des couches de style, $\\alpha$ et $\\beta$, et pourquoi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c23f83",
   "metadata": {
    "editable": false,
    "id": "0ac3d6e4",
    "lang": "en"
   },
   "source": [
    "## Q3F\n",
    "For this case of style transfer, several parameters can be modified to allow to modify the rendering of the hybrid image. The weighting for each of the style layers can be modified as well as the $\\alpha$ and $\\beta$ parameters. Depending on your understanding of the article and your tests, observe the effect of each of these parameters.\n",
    "\n",
    "In the answer cell provided, please answer the following questions:\n",
    "1. What is the effect of weighting the layers differently on the style?\n",
    "2. What happens if you weight the first few layers more heavily?\n",
    "3. What happens if we weight the last layers more heavily?\n",
    "4. What is the effect of the $\\alpha$ parameter?\n",
    "5. What is the effect of the $\\beta$ parameter?\n",
    "6. What do you think is a good configuration of the style layer weight parameters, $\\alpha$ and $\\beta$, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577cdee",
   "metadata": {
    "editable": false,
    "id": "f1e7ddb3",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q3F dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09527aa2",
   "metadata": {
    "editable": false,
    "id": "34d507b1",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q3F in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead28a1",
   "metadata": {
    "deletable": false,
    "id": "38b5c8b6",
    "tags": [
     "user-answer-D4Q3F",
     "editable"
    ]
   },
   "source": [
    "1.pouvoir obtenir des informations nettes ou floutées sur les images\n",
    "\n",
    "2. Avoir des informations détaillées sur les pixels du style\n",
    "\n",
    "3. Avoir des informations de l'image floutée sur le contenu\n",
    "\n",
    "4.$\\alpha$ est le facteur de pondération pour la reconstruction du contenu\n",
    "\n",
    "5.$\\beta$ est le facteur de pondération pour la reconstruction du style\n",
    "\n",
    "6. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "",
  "jupytext": {
   "notebook_metadata_filter": "celltoolbar",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (PAX)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
