{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942ada8a",
   "metadata": {
    "editable": false,
    "id": "412551f6",
    "lang": "fr",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Devoir 4, Question 1 : Réseau de neurones à convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbf5f2",
   "metadata": {
    "editable": false,
    "id": "7d5edcb5",
    "lang": "en",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Homework 4, Question 1: Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57694e0",
   "metadata": {
    "editable": false,
    "id": "e93fa626",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Code préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735517f4",
   "metadata": {
    "editable": false,
    "id": "39112050",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Preamble code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8c3d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-05T00:22:07.282Z"
    },
    "editable": false,
    "id": "dc56e406",
    "tags": [
     "problem-context",
     "autoexec"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import gzip\n",
    "import pandas\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 7.0)\n",
    "from matplotlib import pyplot\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[item[1]] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[val[1]]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cd1bd",
   "metadata": {
    "editable": false,
    "id": "9912bcba",
    "lang": "fr"
   },
   "source": [
    "Pour cette question, vous devez faire l'entraînement d'un réseau de neurones sur le jeu de données [*Volcanoes on Venus*](https://www.kaggle.com/fmena14/volcanoesvenus/). Il s'agit d'un problème de classification pour lequel nous vous fournissons une version abrégée du jeu de données d'[entraînement](https://pax.ulaval.ca/static/GIF-4101-7005/fichiers/volcanoes_train.pt.gz) et de [test](https://pax.ulaval.ca/static/GIF-4101-7005/fichiers/volcanoes_test.pt.gz) (attention: 120 Mo, les fichiers sont directement disponibles sur PAX).\n",
    "\n",
    "Pour vous mettre en contexte, voici une synthèse des trois étapes requises pour l'entraînement de votre modèle dans un contexte de classification:\n",
    "\n",
    "1. **Gérer les données:** La première étape est la gestion des données. Elle se fait en deux temps. En premier, il faut définir une classe (dans le sens *programmation orientée objet* du terme) qui s'occupe des données. Elle permet de contrôler le chargement et l'application des transformations. Chaque fois qu'une donnée (ici une image) est demandée par le système, cette classe est appelée. Cette classe permet de faire une copie des images du disque dur vers la mémoire vive de votre machine. Ce transfert est un moment propice pour appliquer les transformations, tout en réduisant le temps de déplacement des données vers le GPU. Dans *PyTorch*, ceci est effectué par une classe dénommée `Dataset`. Ensuite, il faut définir une classe, nommée `DataLoader` dans *PyTorch*, qui contrôle la façon dont les données sont sélectionnées dans le jeu de données, car on doit pouvoir décider si on veut les piger aléatoirement ou dans un ordre particulier. Elle permet alors de définir la méthode d'échantillonnage et la taille des lots (*batch*) que l'on souhaite obtenir. Notez également que *PyTorch* fonctionne principalement avec des [tenseurs](https://fr.wikipedia.org/wiki/Tenseur) -- généralisation à plusieurs dimensions des matrices.\n",
    "\n",
    "2. **Développer le modèle:** La seconde étape est de définir le modèle de réseau de neurones que l'on souhaite utiliser. Ce réseau peut être construit et personnalisé dans une classe *PyTorch*, que l'on nomme `VolcanoesNet` pour la question courante. Elle permet de définir et d'initialiser les couches du réseau dans la fonction `init`. La fonction `forward` permet de contrôler dans quel ordre se fera l'inférence sur les couches définies dans `init`. Elle permet aussi de varier la forme du tenseur entre les couches, au besoin.\n",
    "\n",
    "3. **Entraîner le modèle:** La dernière étape est d'entraîner le modèle. Pour ce faire, vous devez développer les boucles d'entraînement. On entraîne un modèle itératif, où une époque représente une boucle complète sur toutes les données d'entraînement et une *batch* représente un lot de données utilisées pour une inférence, échantillonnées par le `DataLoader`. Pour chaque *batch*, les opérations d'entraînement d'un réseau (remise à zéro des gradients, inférence, calcul de la perte, rétropropagation) doivent être appliquées. La séparation du jeu de données en *batch* permet de ne pas dépasser la capacité mémoire des GPUs, comme on traite chacune d'entre elles indépendamment. Pour chaque *batch*, les données sont transférées de la mémoire vive du CPU vers le GPU (et inversement à la fin de la *batch*) pour appliquer les opérations d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9930d15",
   "metadata": {
    "editable": false,
    "id": "cfa73dc8",
    "lang": "en"
   },
   "source": [
    "For this question, you have to train a neural network on the [*Volcanoes on Venus*](https://www.kaggle.com/fmena14/volcanoesvenus/) dataset. This is a classification problem for which we provide you with an abridged version of the [training](https://pax.ulaval.ca/static/GIF-4101-7005/fichiers/volcanoes_train.pt.gz) and [testing](https://pax.ulaval.ca/static/GIF-4101-7005/fichiers/volcanoes_test.pt.gz) datasets (warning: 120 MB, the files are directly available on PAX).\n",
    "\n",
    "To put you in context, here is a summary of the three steps required to train your model in a classification context:\n",
    "\n",
    "1. **Data management:** The first step is data management. It is done in two phases. First, you need to define a class (in the *object-oriented programming* meaning of the term) that deals with the data. It allows to control the loading and the application of the transformations. Each time a data element (here an image) is requested by the system, this class is called. This class allows you to make a copy of the images from the hard disk to the RAM of your machine. This transfer is a good time to apply the transformations, while reducing the time taken to move the data to the GPU. In *PyTorch*, this is done by a class called `Dataset`. Next, we need to define a class, named `DataLoader` in *PyTorch*, which controls how the data is selected from the dataset, as we need to be able to decide whether to pick them randomly or in a particular order. It then allows you to define the sampling method and the size of the batches that you want to obtain. Note also that *PyTorch* works mainly with [tensors](https://en.wikipedia.org/wiki/Tensor) -- a generalization to several dimensions of matrices.\n",
    "\n",
    "2. **Model development** The second step is to define the neural network model that you want to use. This network can be built and customized in a *PyTorch* class, which we name `VolcanoesNet` for the current question. It allows to define and initialize the layers of the network with the `init` function. The function `forward` allows to control the order of the inference on the layers defined in `init`. It also allows to vary the shape of the tensor between layers, as needed.\n",
    "\n",
    "3. **Train the model:** The last step is to train the model. To do this, you need to develop the training loops. We train an iterative model, where an epoch represents a complete loop over all the training data and a *batch* represents a bunch of data used for an inference, sampled by the `DataLoader`. For each *batch*, the training operations of a network (gradient reset, inference, loss calculation, backpropagation) must be applied. The separation of the dataset into batches allows to not exceed the memory capacity of the GPUs, as each of them is processed independently. For each batch, the data is transferred from the CPU to the GPU (and vice versa at the end of the *batch*) to apply the training operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44a4d0",
   "metadata": {
    "editable": false,
    "id": "559fc464",
    "lang": "fr"
   },
   "source": [
    "## Q1A\n",
    "Pour commencer, vous vous familiarisez avec les données Volcanoes afin de pouvoir les manipuler dans l'entraînement. Pour ce faire, définissez la classe `VolcanoesDataset`, qui hérite de la classe abstraite `torch.utils.data.Dataset`, et surchargez les méthodes `__getitem__` et `__len__`, comme mentionné dans la [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). Ceci doit résulter en un jeu de données utilisable par *PyTorch*.\n",
    "\n",
    "De plus, vous devez tester votre classe `VolcanoesDataset` en affichant quatre images choisies **aléatoirement**, dans une figure unique. Indiquez la classe correspondante dans le titre de chacune des sous-figures. Également, vous devez représenter la distribution des données par classe du jeu d'entraînement dans un histogramme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b61dc5",
   "metadata": {
    "editable": false,
    "id": "8f10db88",
    "lang": "en"
   },
   "source": [
    "## Q1A \n",
    "To begin, you familiarize yourself with the Volcanoes dataset so that you can manipulate it for the training. To do this, define the class `VolcanoesDataset`, which inherits from the abstract class `torch.utils.data.Dataset`, and overload the methods `__getitem__` and `__len__`, as mentioned in the [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). This should result in a dataset usable by *PyTorch*.\n",
    "\n",
    "In addition, you should test your `VolcanoesDataset` class by displaying four **randomly** chosen images in a single figure. Indicate the corresponding class in the title of each of the subfigures. Also, you need to represent the class distribution of the training set data in a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa218b0",
   "metadata": {
    "editable": false,
    "id": "83a76b4c",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q1A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3702088",
   "metadata": {
    "editable": false,
    "id": "b3210171",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q1A answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5185fe",
   "metadata": {
    "editable": false,
    "id": "60ba73ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VolcanoesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset Volcanoes pour PyTorch\n",
    "    proposé de Francisco Mena sur kaggle : https://bit.ly/2DasPF1\n",
    "\n",
    "    Args:\n",
    "        path (str): le chemin du fichier .pt du dataset\n",
    "\n",
    "    This class is used to define the Volcanoes dataset for PyTorch\n",
    "    proposed by Francisco Mena on kaggle : https://bit.ly/2DasPF1\n",
    "\n",
    "    Args:\n",
    "        path (str): path to dataset .pt file \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        # garde les paramètres en mémoire / store parameters in memory\n",
    "        self.path = path\n",
    "        # charger les données / load data\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            self.data = torch.load(f)\n",
    "        # Pour faciliter la lecture des valeurs cibles / ease reading the targets\n",
    "        self.targets = numpy.array(list(zip(*self.data))[1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # *** TODO ***\n",
    "        # Fourni l'instance à un certain indice du jeu de données\n",
    "        # Provide an instance of the dataset according to the index\n",
    "        pass # Retirer le pass / remove the pass\n",
    "        # ******\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # *** TODO ***\n",
    "        # Fournis la taille du jeu de données\n",
    "        # Provide the lenght of the dataset\n",
    "        pass # Retirer le pass / remove the pass\n",
    "        # ******\n",
    "\n",
    "        \n",
    "# Creation du dataset / Creating the dataset\n",
    "train_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_train.pt.gz')\n",
    "\n",
    "fig, subfigs = pyplot.subplots(2, 2, tight_layout=True)\n",
    "for subfig in subfigs.reshape(-1):\n",
    "    # *** TODO ***\n",
    "    # Affichage de quatre images aléatoires\n",
    "    # Displaying four random images\n",
    "    pass # Retirer le pass / remove the pass\n",
    "    # ******\n",
    "\n",
    "fig, subfig = pyplot.subplots()\n",
    "\n",
    "# *** TODO ***\n",
    "# Tracer histogramme de la distribution des données par classe de train_set\n",
    "# Plot class distribution histogram for train_set\n",
    "# ******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b375ea",
   "metadata": {
    "editable": false,
    "id": "4b887f7b",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q1A dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd6d04",
   "metadata": {
    "editable": false,
    "id": "7700e35d",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q1A in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934f718",
   "metadata": {
    "editable": false,
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "<div class=\"feedback-cell\" style=\"background: rgba(100 , 100 , 100 , 0.4)\">\n",
    "                <h3>Votre soumission a été enregistrée!</h3>\n",
    "                <ul>\n",
    "                    <li>notez qu'il n'y a <strong>pas</strong> de correction automatique pour cet exercice&puncsp;;</li>\n",
    "                    <li>par conséquent, votre note est <strong>actuellement</strong> zéro&puncsp;;</li>\n",
    "                    <li>elle sera cependant ajustée par le professeur dès que la correction manuelle sera complétée&puncsp;;</li>\n",
    "                    <li>vous pouvez soumettre autant de fois que nécessaire jusqu'à la date d'échéance&puncsp;;</li>\n",
    "                    <li>mais évitez de soumettre inutilement.</li>\n",
    "                </ul>\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78883b",
   "metadata": {
    "deletable": false,
    "id": "1add7b51",
    "tags": [
     "user-answer-D4Q1A",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "class VolcanoesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset Volcanoes pour PyTorch\n",
    "    proposé de Francisco Mena sur kaggle : https://bit.ly/2DasPF1\n",
    "\n",
    "    Args:\n",
    "        path (str): le chemin du fichier .pt du dataset\n",
    "\n",
    "    This class is used to define the Volcanoes dataset for PyTorch\n",
    "    proposed by Francisco Mena on kaggle : https://bit.ly/2DasPF1\n",
    "\n",
    "    Args:\n",
    "        path (str): path to dataset .pt file \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        # garde les paramètres en mémoire / store parameters in memory\n",
    "        self.path = path\n",
    "        # charger les données / load data\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            self.data = torch.load(f)\n",
    "        # Pour faciliter la lecture des valeurs cibles / ease reading the targets\n",
    "        self.targets = numpy.array(list(zip(*self.data))[1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # *** TODO ***\n",
    "        # Fourni l'instance à un certain indice du jeu de données\n",
    "        # Provide an instance of the dataset according to the index\n",
    "#         if isinstance(index, slice):\n",
    "#             indices = range(*index.indices(len(self.data)))\n",
    "#             return [self.data[n][0] for n in indices]\n",
    "        return self.data[index][0], self.targets[index]\n",
    "        # ******\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # *** TODO ***\n",
    "        # Fournis la taille du jeu de données\n",
    "        # Provide the lenght of the dataset\n",
    "        return len(self.data)\n",
    "        # ******\n",
    "\n",
    "        \n",
    "# Creation du dataset / Creating the dataset\n",
    "train_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_train.pt.gz')\n",
    "\n",
    "fig, subfigs = pyplot.subplots(2, 2, tight_layout=True)\n",
    "for subfig in subfigs.reshape(-1):\n",
    "    # *** TODO ***\n",
    "    # Affichage de quatre images aléatoires\n",
    "    # Displaying four random images\n",
    "    subfig.axis(\"off\")\n",
    "    subfig.imshow(train_set[numpy.random.randint(7000, size=1)[0]][0].permute(1, 2, 0), cmap=\"gray_r\")\n",
    "    # ******\n",
    "\n",
    "fig, subfig = pyplot.subplots()\n",
    "\n",
    "# *** TODO ***\n",
    "# Tracer histogramme de la distribution des données par classe de train_set\n",
    "# Plot class distribution histogram for train_set\n",
    "# ******\n",
    "y = train_set.targets\n",
    "labels, counts = numpy.unique(y, return_counts=True)\n",
    "subfig.hist(labels, bins=[-0.5, 0.5, 1.5], weights=counts)\n",
    "subfig.set_xticks(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f111f6b",
   "metadata": {
    "editable": false,
    "id": "bd194a04",
    "lang": "fr"
   },
   "source": [
    "## Q1B\n",
    "Vous devez maintenant créer le réseau de neurones et définir les méthodes ainsi que les attributs nécessaires pour qu'il puisse être entraîné.\n",
    "- Commencez par initialiser les couches de votre réseau dans la méthode `__init__` de `VolcanoesNet`, en utilisant les couches de convolution (`Conv2D`), de normalisation (`BatchNorm2D`) et linéaire (`Linear`), selon l'architecture suivante.\n",
    "![Architecture de VolcanoesNet](https://pax.ulaval.ca/static/GIF-4101-7005/images/d4q1_volcanoes_net.png)\n",
    "- Pour les convolutions, vous devez respecter le nombre de filtres (*filters*) et la taille des noyaux (*kernels*) de convolution. Vous devez aussi, et ce pour toutes les convolutions, spécifier un pas (*stride*) de 2. Aussi, vous devez retirer le biais de la convolution si cette dernière est suivie d'une couche de normalisation, car elle contient déjà un paramètre pour le biais.\n",
    "- Écrivez les lignes de code manquantes pour définir l'ordre d'inférence des couches dans le réseau dans la méthode `forward` de `VolcanoesNet`. Les modules `average_pooling`, `linear` et `sigmoid` sont déjà implémentées dans la librairie [PyTorch](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e34ba",
   "metadata": {
    "editable": false,
    "id": "3ba5871e",
    "lang": "en"
   },
   "source": [
    "## Q1B\n",
    "You now need to create the neural network and define the methods and attributes needed to train it.\n",
    "- Start by initializing the layers of your network in the `__init__` method of `VolcanoesNet`, using the convolution (`Conv2D`), normalization (`BatchNorm2D`) and linear (`Linear`) layers, according to the following architecture.\n",
    "![VolcanoesNet Architecture](https://pax.ulaval.ca/static/GIF-4101-7005/images/d4q1_volcanoes_net.png)\n",
    "- For convolutions, you must respect the number of filters and the size of the convolution kernels. You must also, for all convolutions, specify a stride of 2. Also, you must remove the bias from the convolution if it is followed by a normalization layer, since it already contains a parameter for the bias.\n",
    "- Write the missing lines of code to define the order of inference of the layers in the network in the `forward` method of `VolcanoesNet`. The modules `average_pooling`, `linear` and `sigmoid` are already implemented in the [PyTorch] library (https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc722c",
   "metadata": {
    "editable": false,
    "id": "7b82a31d",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5d9b0",
   "metadata": {
    "editable": false,
    "id": "2d662b51",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q1B answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4da823",
   "metadata": {
    "editable": false,
    "id": "0e202638",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VolcanoesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau pleinement convolutionnel simple\n",
    "    permettant de classifier des images satellite de Venus.\n",
    "    This class defines a simple fully convolutional network\n",
    "    to classify satellite images from Venus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # *** TODO ***\n",
    "        # Initialiser ici les modules contenant des paramètres à optimiser.\n",
    "        # Ces modules seront utilisés dans la méthode 'forward'\n",
    "        pass # Retirer le pass / Remove the pass\n",
    "        # ******\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # *** TODO ***\n",
    "        # Effectuer l'inférence du réseau. L'ordre d'exécution est importante.\n",
    "        # Perform network inference. The order of execution is important.\n",
    "        return False  # Retourner la bonne valeur / return the right value\n",
    "        # ******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70301edf",
   "metadata": {
    "editable": false,
    "id": "2030e4d6",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q1B dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a52b07",
   "metadata": {
    "editable": false,
    "id": "b3f20259",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q1B in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50d551",
   "metadata": {
    "deletable": false,
    "id": "e66ea5c8",
    "tags": [
     "user-answer-D4Q1B",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "class VolcanoesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau pleinement convolutionnel simple\n",
    "    permettant de classifier des images satellite de Venus.\n",
    "    This class defines a simple fully convolutional network\n",
    "    to classify satellite images from Venus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # *** TODO ***\n",
    "        # Initialiser ici les modules contenant des paramètres à optimiser.\n",
    "        # Ces modules seront utilisés dans la méthode 'forward'\n",
    "        super().__init__()\n",
    "        self.C1 = nn.Conv2d(1, 32, kernel_size=5, stride=2, bias=False)\n",
    "        self.B2 = nn.BatchNorm2d(32)\n",
    "        self.C3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.B4 = nn.BatchNorm2d(64)\n",
    "        self.C5 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.B6 = nn.BatchNorm2d(64)\n",
    "        self.C7 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.B8 = nn.BatchNorm2d(64)\n",
    "        self.C9 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.B10 = nn.BatchNorm2d(64)\n",
    "        self.A11 = nn.AvgPool2d(2)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        # ******\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # *** TODO ***\n",
    "        # Effectuer l'inférence du réseau. L'ordre d'exécution est importante.\n",
    "        # Perform network inference. The order of execution is important.\n",
    "        x = F.relu(self.B2(self.C1(x)))\n",
    "        x = F.relu(self.B4(self.C3(x)))\n",
    "        x = F.relu(self.B6(self.C5(x)))\n",
    "        x = F.relu(self.B8(self.C7(x)))\n",
    "        x = self.B10(self.C9(x))\n",
    "        x = self.A11(x)\n",
    "        x = x.view(-1, 64)\n",
    "        return torch.sigmoid(self.output(x))  # Retourner la bonne valeur / return the right value\n",
    "        # ******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd522939",
   "metadata": {
    "editable": false,
    "id": "b322b99d",
    "lang": "fr"
   },
   "source": [
    "## Q1C\n",
    "Il faut maintenant développer les outils nécessaires pour effectuer l'entraînement du réseau de neurones, selon le code que vous avez développé aux sous-questions précédentes. L'entraînement est défini par une boucle qui itère sur l'ensemble des données d'entraînement, chaque itération correspondant à une époque. Pour chaque époque, il faut itérer sur tous les lots (*batch*) qu'elle contient.\n",
    "\n",
    "Pour cette question, vous devez:\n",
    " - Écrire le code manquant pour la préparation de l'entraînement.\n",
    " - Écrire le code manquant à l'intérieur de la boucle d'entraînement.\n",
    " - Écrire le code manquant à l'intérieur de la fonction de calcul de l'erreur et des matrices de confusion.\n",
    "\n",
    "La matrice de confusion est particulièrement utile pour visualiser les performances de votre réseau. On assigne la donnée à la première classe 0 lorsque la probabilité en sortie est plus petite que 0,5, sinon la données est assignée à la deuxième classe. \n",
    "\n",
    "Également, discutez brièvement les performances du réseau selon la matrice de confusion obtenue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c62cc9",
   "metadata": {
    "editable": false,
    "id": "3923b461",
    "lang": "en"
   },
   "source": [
    "## Q1C\n",
    "We now need to develop the tools necessary to perform the training of the neural network, according to the code you developed in the previous sub-questions. The training is defined by a loop that iterates over the whole training data, each iteration corresponding to an epoch. For each epoch, you must iterate over all the batches it contains.\n",
    "\n",
    "For this question, you must:\n",
    " - Write the missing code for the training setup.\n",
    " - Write the missing code inside the training loop.\n",
    " - Write the missing code inside the error calculation function and confusion matrices.\n",
    "\n",
    "The confusion matrix is particularly useful for visualizing the performance of your network. The data is assigned to the first class 0 when the output probability is smaller than 0.5, otherwise the data is assigned to the second class. \n",
    "\n",
    "Also, briefly discuss the performance of the network according to the obtained confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63753c22",
   "metadata": {
    "editable": false,
    "id": "e0204f07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialisation des paramètres d'entraînement\n",
    "# Paramètres recommandés:\n",
    "# - Nombre d'epochs (nb_epoch = 10)\n",
    "# - Taux d'apprentissage (learning_rate = 0.01)\n",
    "# - Momentum (momentum = 0.9)\n",
    "# - Taille du lot (batch_size = 32)\n",
    "#\n",
    "# Initialization of training parameters\n",
    "# Recommended parameters:\n",
    "# - Number of epochs (nb_epoch = 10)\n",
    "# - Learning rate (learning_rate = 0.01)\n",
    "# - Momentum (momentum = 0.9)\n",
    "# - Batch size (batch_size = 32)\n",
    "nb_epoch = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "\n",
    "# Chargement des données d'entraînement et de test\n",
    "# Loading training and testing set\n",
    "train_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_train.pt.gz')\n",
    "test_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_test.pt.gz')\n",
    "\n",
    "# Création du sampler avec les classes balancées\n",
    "# Create the sampler with balanced classes\n",
    "balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "# Création du dataloader d'entraînement\n",
    "# Create training dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "def compute_confusion_matrix(model, dataloader, device):\n",
    "    \n",
    "    # *** TODO ***\n",
    "    # Mettre le model en mode évaluation\n",
    "    # Calculer toutes les prédictions sur le dataloader\n",
    "    # Put the model in evaluation mode\n",
    "    # Compute all predictions on the dataloader  \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    predictions_numpy = numpy.concatenate(all_predictions)\n",
    "    targets_numpy = numpy.concatenate(all_targets)\n",
    "    # ******\n",
    "\n",
    "    # *** TODO ***\n",
    "    # Assigner la classe 0 ou 1 aux prédictions\n",
    "    # Calculer la matrice de confusion. Attention de bien avoir\n",
    "    # une matrice 2 par 2 en sortie\n",
    "    #\n",
    "    # Assign class 0 or 1 to the predictions\n",
    "    # Compute the confusion matrix. Be careful to have\n",
    "    # a 2 by 2 matrix as output.\n",
    "    # ******\n",
    "\n",
    "    return matrix  # Retourner matrice de confusion / return confusion matrix\n",
    "\n",
    "\n",
    "# *** TODO ***\n",
    "# Instancier votre réseau VolcanoesNet dans une variable nommée \"model\"\n",
    "# Instantiate your VolcanoesNet network in a variable named \"model\"\n",
    "# ******\n",
    "\n",
    "# Transférer le réseau sur GPU ou CPU en fonction de la variable \"DEVICE\"\n",
    "# Transfer the network to GPU or CPU depending on the \"DEVICE\" variable\n",
    "model.to(DEVICE)\n",
    "\n",
    "# *** TODO ***\n",
    "# Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "# et la mettre dans une variable nommée criterion\n",
    "# Instantiate an error function BinaryCrossEntropy\n",
    "# and put it in a variable named criterion\n",
    "\n",
    "# Instancier l'algorithme d'optimisation SGD\n",
    "# Ne pas oublier de lui donner les hyperparamètres\n",
    "# d'entraînement : learning rate et momentum!\n",
    "# Instantiate the SGD optimization algorithm\n",
    "# Don't forget to give it the training hyperparameters:\n",
    "# learning rate and momentum!\n",
    "\n",
    "# Mettre le réseau en mode entraînement\n",
    "# Set the network in training mode\n",
    "# ******\n",
    "\n",
    "# Boucle d'entraînement / Training loop\n",
    "for i_epoch in range(nb_epoch):\n",
    "\n",
    "    start_time, train_losses = time.time(), []\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        images, targets = batch\n",
    "        targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Mettre les gradients à zéro\n",
    "        # Set gradients to zero\n",
    "\n",
    "        # Calculer:\n",
    "        # 1. l'inférence dans une variable \"predictions\"\n",
    "        # 2. l'erreur dans une variable \"loss\"\n",
    "        # Compute:\n",
    "        # 1. the inference in a \"predictions\" variable\n",
    "        # 2. the error in a \"loss\" variable\n",
    "\n",
    "        # Rétropropager l'erreur et effectuer\n",
    "        # une étape d'optimisation\n",
    "        # Backpropagate the error and perform\n",
    "        # an optimization step\n",
    "        # ******\n",
    "        \n",
    "        # Accumulation du loss de la batch\n",
    "        # Accumulating batch loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "        i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "# Affichage du score en test / Display test score\n",
    "test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "\n",
    "# Affichage de la matrice de confusion / Display confusion matrix\n",
    "matrix = compute_confusion_matrix(model, test_loader, DEVICE)\n",
    "print(matrix)\n",
    "\n",
    "# Libère la cache sur le GPU *important sur un cluster de GPU*\n",
    "# Free GPU cache *important on a GPU cluster*\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# *** TODO ***\n",
    "# Entrez vos commentaires de la discussion ici.\n",
    "# Enter your discussion comments here\n",
    "discussion = \"Entrez vos commentaires de la discussion ici.\"\n",
    "# ******\n",
    "\n",
    "frame = {\"Comments\":[discussion]}\n",
    "df = pandas.DataFrame(frame)\n",
    "display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8715a",
   "metadata": {
    "editable": false,
    "id": "8e99ec59",
    "lang": "fr"
   },
   "source": [
    "### Entrez votre solution à Q1C dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09474953",
   "metadata": {
    "editable": false,
    "id": "456025fa",
    "lang": "en"
   },
   "source": [
    "### Enter your answer to Q1C in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff39e8",
   "metadata": {
    "deletable": false,
    "id": "451db74c",
    "tags": [
     "user-answer-D4Q1C",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialisation des paramètres d'entraînement\n",
    "# Paramètres recommandés:\n",
    "# - Nombre d'epochs (nb_epoch = 10)\n",
    "# - Taux d'apprentissage (learning_rate = 0.01)\n",
    "# - Momentum (momentum = 0.9)\n",
    "# - Taille du lot (batch_size = 32)\n",
    "#\n",
    "# Initialization of training parameters\n",
    "# Recommended parameters:\n",
    "# - Number of epochs (nb_epoch = 10)\n",
    "# - Learning rate (learning_rate = 0.01)\n",
    "# - Momentum (momentum = 0.9)\n",
    "# - Batch size (batch_size = 32)\n",
    "nb_epoch = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "\n",
    "# Chargement des données d'entraînement et de test\n",
    "# Loading training and testing set\n",
    "train_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_train.pt.gz')\n",
    "test_set = VolcanoesDataset('/pax/shared/GIF-4101-7005/volcanoes_test.pt.gz')\n",
    "\n",
    "# Création du sampler avec les classes balancées\n",
    "# Create the sampler with balanced classes\n",
    "balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "# Création du dataloader d'entraînement\n",
    "# Create training dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "def compute_confusion_matrix(model, dataloader, device):\n",
    "    \n",
    "    # *** TODO ***\n",
    "    # Mettre le model en mode évaluation\n",
    "    # Calculer toutes les prédictions sur le dataloader\n",
    "    # Put the model in evaluation mode\n",
    "    # Compute all predictions on the dataloader\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    predictions_numpy = numpy.concatenate(all_predictions)\n",
    "    targets_numpy = numpy.concatenate(all_targets)\n",
    "    # ******\n",
    "\n",
    "    # *** TODO ***\n",
    "    # Assigner la classe 0 ou 1 aux prédictions\n",
    "    # Calculer la matrice de confusion. Attention de bien avoir\n",
    "    # une matrice 2 par 2 en sortie\n",
    "    #\n",
    "    # Assign class 0 or 1 to the predictions\n",
    "    # Compute the confusion matrix. Be careful to have\n",
    "    # a 2 by 2 matrix as output.\n",
    "    # ******\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "        \n",
    "    matrix = numpy.array([\n",
    "     numpy.sum(numpy.logical_and(predictions_numpy == 0, targets_numpy == 0)), \n",
    "     numpy.sum(numpy.logical_and(predictions_numpy == 0, targets_numpy == 1)),\n",
    "     numpy.sum(numpy.logical_and(predictions_numpy == 1, targets_numpy == 0)),\n",
    "     numpy.sum(numpy.logical_and(predictions_numpy == 1, targets_numpy == 1)),\n",
    "    ]).reshape((2, 2))\n",
    "\n",
    "    return matrix  # Retourner matrice de confusion / return confusion matrix\n",
    "\n",
    "\n",
    "# *** TODO ***\n",
    "# Instancier votre réseau VolcanoesNet dans une variable nommée \"model\"\n",
    "# Instantiate your VolcanoesNet network in a variable named \"model\"\n",
    "# ******\n",
    "model = VolcanoesNet()\n",
    "\n",
    "# Transférer le réseau sur GPU ou CPU en fonction de la variable \"DEVICE\"\n",
    "# Transfer the network to GPU or CPU depending on the \"DEVICE\" variable\n",
    "model.to(DEVICE)\n",
    "\n",
    "# *** TODO ***\n",
    "# Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "# et la mettre dans une variable nommée criterion\n",
    "# Instantiate an error function BinaryCrossEntropy\n",
    "# and put it in a variable named criterion\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Instancier l'algorithme d'optimisation SGD\n",
    "# Ne pas oublier de lui donner les hyperparamètres\n",
    "# d'entraînement : learning rate et momentum!\n",
    "# Instantiate the SGD optimization algorithm\n",
    "# Don't forget to give it the training hyperparameters:\n",
    "# learning rate and momentum!\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Mettre le réseau en mode entraînement\n",
    "# Set the network in training mode\n",
    "# ******\n",
    "model.train()\n",
    "\n",
    "# Boucle d'entraînement / Training loop\n",
    "for i_epoch in range(nb_epoch):\n",
    "\n",
    "    start_time, train_losses = time.time(), []\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        images, targets = batch\n",
    "        targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Mettre les gradients à zéro\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculer:\n",
    "        # 1. l'inférence dans une variable \"predictions\"\n",
    "        # 2. l'erreur dans une variable \"loss\"\n",
    "        # Compute:\n",
    "        # 1. the inference in a \"predictions\" variable\n",
    "        # 2. the error in a \"loss\" variable\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # Rétropropager l'erreur et effectuer\n",
    "        # une étape d'optimisation\n",
    "        # Backpropagate the error and perform\n",
    "        # an optimization step\n",
    "        # ******\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulation du loss de la batch\n",
    "        # Accumulating batch loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "        i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "# Affichage du score en test / Display test score\n",
    "test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "\n",
    "# Affichage de la matrice de confusion / Display confusion matrix\n",
    "matrix = compute_confusion_matrix(model, test_loader, DEVICE)\n",
    "print(matrix)\n",
    "\n",
    "# Libère la cache sur le GPU *important sur un cluster de GPU*\n",
    "# Free GPU cache *important on a GPU cluster*\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# *** TODO ***\n",
    "# Entrez vos commentaires de la discussion ici.\n",
    "# Enter your discussion comments here\n",
    "discussion = \"Je constate que le résultat présenté par la matrice de confusion\\\n",
    "              est légèrement supérieur au résultat affiché par test_acc\"\n",
    "# ******\n",
    "\n",
    "frame = {\"Comments\":[discussion]}\n",
    "df = pandas.DataFrame(frame)\n",
    "display.display(df)"
   ]
  }
 ],
 "metadata": {
  "PAX": {
   "userLang": "fr"
  },
  "celltoolbar": "",
  "jupytext": {
   "notebook_metadata_filter": "celltoolbar",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (PAX)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "fr"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
